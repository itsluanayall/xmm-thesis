import logging
import os
from datetime import datetime as dt
from astropy.io import fits, ascii
from astropy.table import Table
from astropy.time import Time
import glob
import pandas as pd
from config import CONFIG
from tools import *
import numpy as np
import matplotlib.pyplot as plt
import numpy as np
from itertools import compress
from scipy.optimize import curve_fit
import xspec
import sys

class Exposure:
    """
    Class for an Exposure of an Observation.
    """

    def __init__(self, evenli, srcli='srcli', specli="specli", bkgli="bkgli", respli="respli"):
        """
        Constructor. evenli is the Eventlist of the exposure, which contains all the information
        that will be stored as attributes of the Exposure (RGS ONLY).

        :param evenli: name of the RGS eventlist generated by SAS 
        :type evenli: str
        :param srcli: name of the sourcelist generated by SAS rgsproc, optional
        :type srcli: str
        :param specli: name of the RGS spectrum, optional
        :type specli: str
        :param bkgli: name of the RGS background, optional
        :type bkgli: str
        :param respli: name of the RGS response matrix, optional
        :type respli: str

        """
        self.evenli = evenli
        self.srcli = srcli
        self.specli = specli
        self.bkgli = bkgli
        self.respli = respli

        with fits.open(evenli) as hdul:
            self.obsid = hdul[1].header['OBS_ID']
            self.expid = str(hdul[1].header['EXP_ID']).split(self.obsid)[1]
            self.fullid = hdul[1].header['EXP_ID']
            self.instrume = hdul[1].header['INSTRUME']
            self.tstart = hdul[1].header['TSTART']
            self.tstop = hdul[1].header['TSTOP']
            self.telapse = hdul[1].header['TELAPSE']
            self.start_date_str = hdul[1].header['DATE-OBS']
            self.end_date_str = hdul[1].header['DATE-END']


    def synchronous_times(self, exp2):
        """
        Compares the exposure's start and stop times to those of another exposure. If the exposures overlap,
        the method returns the start time and the stop times of the overlapping interval.

        :param exp2: exposure you want to compare self with.
        :type exp2: class Exposure
        ...
        :raises Exception: when the two exposures are not synchronous
        ...
        :return: two floats, representing the start time and stop time for the pair of exposures

        """
        final_start = max(self.tstart, exp2.tstart)
        final_stop = min(self.tstop, exp2.tstop)
        overlap = max(0., final_stop-final_start)

        try:
            if overlap>0:
                return final_start, final_stop
            else:
                raise Exception
        except Exception as e:
            logging.error("The given exposures do not overlap. Please check the if the input exposures are correct.")


class Observation:
    """
    Observation class for a specific target.
    In order to avoid unnecessary complexity, the method names of the class recall
    the SAS commands used to analyse observations.
    """
    def __init__(self, obsid, target_dir):
        """
        Constructor. The target directory and the observation ID must be passed as arguments.
        The other attributes are then made based on these arguments.

        :param obsid: Observation ID
        :type obsid: str
        :param target_dir: directory contaning the observations of the target
        :type target_dir: str

        """
        self._target_dir = target_dir
        self._obsid = obsid
        self._obsdir = os.path.join(self.target_dir, self.obsid) 
        self._odfdir = os.path.join(self.obsdir, 'odf')
        self._rgsdir = os.path.join(self.obsdir, 'rgs')
        self._emdir = os.path.join(self.obsdir, 'epic', 'mos')
        self._epdir = os.path.join(self.obsdir, 'epic', 'pn')
        
        #The following attributes will be modified in the class methods. 
        # For now, we just inizialize them
        self.revolution = 0    
        self.starttime = 0.
        self.endtime = 0.
        self.duration = 0.
        self.rgsrate = []
        self.epicrate = []
        self.epic_erate = []
        self.stdev = []
        self.fracvardict = []
        self.expoid = []
        self.epic_expid = 0.
        self.longterm_lc_times = []
        self.duration_lc_ks = []

    @property
    def target_dir(self):
        return self._target_dir
    @property
    def obsid(self):
        return self._obsid
    @property
    def obsdir(self):
        return self._obsdir
    @property
    def odfdir(self):
        return self._odfdir
    @property
    def rgsdir(self):
        return self._rgsdir
    @property
    def emdir(self):
        return self._emdir
    @property
    def epdir(self):
        return self._epdir

    def create_pairs_exposures(self):
        """
        Defines lists of pairs of the RGS exposures of the observation. Some observations (0510610101, 0510610201, 0136540701) 
        pairs are defined by hand because these observations were interrupted and so their exposures do not 
        follow the standard conventions.
        """
        os.chdir(self.rgsdir)

        #Sort RGS eventlists according to exposure number
        self.pairs_events = sort_rgs_list(self.rgsevlists, 'expo_number')

        if self.obsid=='0510610101':
            self.pairs_events = [['P0510610101R1S004EVENLI0000.FIT', 'P0510610101R2S005EVENLI0000.FIT'], ['P0510610101R1S004EVENLI0000.FIT', 'P0510610101R2S013EVENLI0000.FIT']]
        if self.obsid=='0510610201':
            self.pairs_events = [['P0510610201R1S004EVENLI0000.FIT', 'P0510610201R2S005EVENLI0000.FIT'], ['P0510610201R1S015EVENLI0000.FIT', 'P0510610201R2S005EVENLI0000.FIT']]
        if self.obsid=='0136540701':
            self.pairs_events = [['P0136540701R1S001EVENLI0000.FIT','P0136540701R2S002EVENLI0000.FIT' ], ['P0136540701R1S001EVENLI0000.FIT', 'P0136540701R2S018EVENLI0000.FIT']]
        self.npairs = len(self.pairs_events)
        logging.info(f'There is(are) {self.npairs} set(s) of exposures for observation {self.obsid}.')
        print(self.pairs_events)

        #Sort RGS sourcelists according to exposure number
        self.pairs_srcli = sort_rgs_list(self.rgssrclists, 'expo_number')

        if self.obsid=='0510610101':
            self.pairs_srcli = [['P0510610101R1S004SRCLI_0000.FIT', 'P0510610101R2S005SRCLI_0000.FIT'], ['P0510610101R1S004SRCLI_0000.FIT', 'P0510610101R2S013SRCLI_0000.FIT']]
        if self.obsid=='0510610201':
            self.pairs_srcli = [['P0510610201R1S004SRCLI_0000.FIT', 'P0510610201R2S005SRCLI_0000.FIT'], ['P0510610201R1S015SRCLI_0000.FIT', 'P0510610201R2S005SRCLI_0000.FIT']]
        if self.obsid=='0136540701':
            self.pairs_srcli = [['P0136540701R1S001SRCLI_0000.FIT', 'P0136540701R2S002SRCLI_0000.FIT'], ['P0136540701R1S001SRCLI_0000.FIT', 'P0136540701R2S018SRCLI_0000.FIT']]
        

    def cifbuild(self):
        """
        Generates the Calibration Index File (CIF) for the observation
        in the directory of the observation, and sets the 'SAS_CCF' variable pointing to it.
        """
        #Point SAS_ODF to the odf of the current observation 
        os.environ["SAS_ODF"] = self.odfdir
        os.chdir(self.obsdir)

        #Run the SAS command to make the ccf.cif file (output in logfile)
        if not glob.glob('ccf.cif'):    #check if the ccf file hasn't already been processed
            logging.info(f'Building CIF file for observation number {self.obsid}')
            ccf_command = "cifbuild > my_cifbuild_logfile"
            ccf_status = run_command(ccf_command)
            if (ccf_status != 0):
                raise Exception    
            else:
                logging.info(f"CIF file created for observation number {self.obsid}")
        else:
            logging.info(f'CIF file for observation number {self.obsid} already exists.')

        #Set SAS_CCF variable pointing to the product of 'cifbuild'
        os.environ["SAS_CCF"] = os.path.join(self.obsdir, 'ccf.cif')
        logging.info(f"SAS_CCF pointing to {os.path.join(self.obsdir, 'ccf.cif')}")


    def odfingest(self):
        """
        Generates the SUM.ASC file in the observation directory,
        that summarizes all the observational info
        and sets 'SAS_ODF' variable pointing to it.
        """
        #Point the SAS_ODF environment variable to the odf of the current observation
        os.environ["SAS_ODF"] = self.odfdir
        os.chdir(self.obsdir)

        #Run the SAS command odfingest (output in logfile)
        if not glob.glob('*SUM.SAS'):
            logging.info(f'Building *SUM.SAS file for observation number {self.obsid}')
            odf_command = "odfingest > my_odfingest_logfile"
            odf_status = run_command(odf_command)
            if (odf_status != 0):
                raise Exception
            else:
                logging.info(f"*SUM.SAS file created for observation number {self.obsid}")
        else:
            logging.info(f'*SUM.SAS file for observation number {self.obsid} already exists.')

        #Set SAS_ODF variable pointing to the product of 'odfingest'
        sum_odf_dir = glob.glob('*SUM.SAS')[0]
        os.environ["SAS_ODF"] = os.path.join(self.obsdir, sum_odf_dir)
        logging.info(f"SAS_ODF pointing to {os.path.join(self.obsdir, sum_odf_dir)}")

        # Mark info of the observation and assign Revolution Identifier, Observation Start time and End time
        # to attributes of the class. To move in a new method (?)
        with open(sum_odf_dir) as f:    
            sum_odf = f.read()

        # retrieve duration of observation from summary file generated 
        sum_odf = sum_odf.split('\n') #divide text file into lines
        for line in sum_odf:
            if not line.startswith('//'): #skip commented lines
                if line.endswith('Revolution Identifier'):
                    self.revolution = line.split('/')[0]
                if line.endswith('Observation Start Time'):
                    self.starttime = Time(line.split('/')[0], format='isot', scale='utc')
                if line.endswith('Observation End Time'):
                    self.endtime = Time(line.split('/')[0], format='isot', scale='utc')
        self.duration = ((self.endtime - self.starttime)*86400).value    #duration observation in seconds


    def epproc(self):
        """
        Runs the epproc SAS command to process and reduce EPIC-PN data.
        """
        os.chdir(self.epdir)

        if not glob.glob('*TimingEvts.ds'):
            logging.info(f'Running epproc command for observation number {self.obsid}...')
            epicpn_command = f'epproc'
            epicpn_status = run_command(epicpn_command)
            if (epicpn_status != 0):
                print(f'\033[91m An error has occurred running epproc for observation {self.obsid}! \033[0m')
            else:
                logging.info(f'Done processing EPIC-PN data. The products are in {self.epdir}.')
        
        else:
            logging.info(f'EPIC-PN event lists for observation number {self.obsid} already exist.')
        
        with fits.open(glob.glob('*TimingEvts.ds')[0]) as hdul:
            self.epic_expid = hdul[1].header['EXPIDSTR']


    def epiclccorr(self, pileup=False):
        """
        Extracts a source and background raw lightcurve for EPIC-pn and runs epiclccorr to correct the raw lightcurve.
        """
        os.chdir(self.epdir)       

        if not glob.glob(f"PN_soft.lc"):

            try:
                with open("ds9.reg") as f:
                    region = f.read()

            except FileNotFoundError as e:
                logging.ERROR("Please extract manually source and background coordinates into a .reg file using ds9 software.")
                sys.exit(0)
                
            region = region.split('\n') #divide text file into lines

            #Source coordinates
            coordinates = region[3][4:-1].split(',')
            xcenter = float(coordinates[0])
            xmax = xcenter + float(coordinates[2])/2
            xmin = xcenter - float(coordinates[2])/2

            #Background coordinates
            coordinates_bkg = region[4][4:-1].split(',')
            xcenter_bkg = float(coordinates_bkg[0])
            xmax_bkg = xcenter_bkg + float(coordinates_bkg[2])/2
            xmin_bkg = xcenter_bkg - float(coordinates_bkg[2])/2

            #Soft lightcurve
            logging.info(f"Extracting Source+Background soft lightcurve (0.6 - 2 keV) for EPIC-PN...")
            
            if pileup:
                evselect_source_cmmd = f"evselect table=PNclean.fits energycolumn=PI expression='#XMMEA_EP && (PATTERN<=4) && (RAWX in [{xmin}:{xmax}]) &&! (RAWX in [{xcenter-1}:{xcenter+1}]) && (PI in [600:2000])' withrateset=yes rateset='PN_soft_raw.lc' timebinsize={self.epic_timebinsize} maketimecolumn=yes makeratecolumn=yes"
            else:
                evselect_source_cmmd = f"evselect table=PNclean.fits energycolumn=PI expression='#XMMEA_EP && (PATTERN<=4) && (RAWX in [{xmin}:{xmax}]) && (PI in [200:2000])' withrateset=yes rateset='PN_soft_raw.lc' timebinsize={self.epic_timebinsize} maketimecolumn=yes makeratecolumn=yes"
               
            evselect_source_status = run_command(evselect_source_cmmd)

            evselect_bkg_cmmd = f"evselect table=PNclean.fits energycolumn=PI expression='#XMMEA_EP && (PATTERN<=4) && (RAWX>={xmin_bkg}) && (RAWX<={xmax_bkg}) && (PI in [600:2000])' withrateset=yes rateset='PN_bkg_soft_raw.lc' timebinsize={self.epic_timebinsize} maketimecolumn=yes makeratecolumn=yes"
            evselect_bkg_status = run_command(evselect_bkg_cmmd)

            logging.info(f'Running epiclccorr for soft lightcurve of EPIC-PN...')
            epiclccorr_soft_cmmd = f"epiclccorr srctslist=PN_soft_raw.lc eventlist=PNclean.fits outset=PN_soft.lc bkgtslist=PN_bkg_soft_raw.lc withbkgset=yes applyabsolutecorrections=yes"
            epiclccorr_soft_status = run_command(epiclccorr_soft_cmmd)

            #Hard lightcurve
            logging.info(f"Extracting Source+Background hard lightcurve (2-10 keV) for EPIC-PN...")
            
            if pileup:
                evselect_source_cmmd = f"evselect table=PNclean.fits energycolumn=PI expression='#XMMEA_EP && (PATTERN<=4) && (RAWX in [{xmin}:{xmax}]) &&! (RAWX in [{xcenter-1}:{xcenter+1}]) && (PI in [2000:10000])' withrateset=yes rateset='PN_hard_raw.lc' timebinsize={self.epic_timebinsize} maketimecolumn=yes makeratecolumn=yes"
            else:
                evselect_source_cmmd = f"evselect table=PNclean.fits energycolumn=PI expression='#XMMEA_EP && (PATTERN<=4) && (RAWX in [{xmin}:{xmax}]) && (PI in [2000:10000])' withrateset=yes rateset='PN_hard_raw.lc' timebinsize={self.epic_timebinsize} maketimecolumn=yes makeratecolumn=yes"
 
            evselect_source_status = run_command(evselect_source_cmmd)

            evselect_bkg_cmmd = f"evselect table=PNclean.fits energycolumn=PI expression='#XMMEA_EP && (PATTERN<=4) && (RAWX>={xmin_bkg}) && (RAWX<={xmax_bkg}) && (PI in [2000:10000])' withrateset=yes rateset='PN_bkg_hard_raw.lc' timebinsize={self.epic_timebinsize} maketimecolumn=yes makeratecolumn=yes"
            evselect_bkg_status = run_command(evselect_bkg_cmmd)

            logging.info(f'Running epiclccorr for hard lightcurve of EPIC-PN...')
            epiclccorr_hard_cmmd = f"epiclccorr srctslist=PN_hard_raw.lc eventlist=PNclean.fits outset=PN_hard.lc bkgtslist=PN_bkg_hard_raw.lc withbkgset=yes applyabsolutecorrections=yes"
            epiclccorr_hard_status = run_command(epiclccorr_hard_cmmd)

            logging.info('Done running epiclccor for EPIC-PN!')
        else:
            logging.info('Already extracted PN lightcurve.')


    def rgsproc(self):
        """
        Runs the rgsproc SAS command to process and reduce RGS data. 
        """
        os.chdir(self.rgsdir)

        #Check if the data has already been processed: if not, run the command.
        
        if not glob.glob('*EVENLI0000.FIT'):
            logging.info(f'Running rgsproc command for observation number {self.obsid}...')
            ra = CONFIG['target_RA']
            dec = CONFIG['target_DEC']
            rgs_command = f'rgsproc withsrc=yes srclabel=USER srcra={ra} srcdec=+{dec} > my_rgsproc_logfile'
            rgs_status = run_command(rgs_command)
            if (rgs_status != 0):
                print(f'\033[91m An error has occurred running rgsproc for observation {self.obsid}! \033[0m')
            else:
                logging.info(f'Done processing RGS data. The products are in {self.rgsdir}.')
        else:
            logging.info(f'RGS event lists for observation number {self.obsid} already exist.')
        
        #Define the products as new attributes of the Observation
        self.rgsevlists = glob.glob('*EVENLI0000.FIT')
        self.rgssrclists = glob.glob('*SRCLI_0000.FIT')

        
    def rgslccorr(self):
        """
        Runs the rgslccorr SAS command for each pair (RGS1+RGS2) of exposures present in the observation.
        The products are the lightcurves .ds files.
        For the observations 0510610101, 0510610201 and 013654701 the eventlists are written manually because
        their exposures do not copme in pairs.
        """
        os.chdir(self.rgsdir)
        
        
        for i in range(self.npairs):
            
            # Istance of the exposures
            expos0 = Exposure(self.pairs_events[i][0], self.pairs_srcli[i][0])
            expos1 = Exposure(self.pairs_events[i][1], self.pairs_srcli[i][1])
            self.expoid.append([expos0.expid, expos1.expid])
            
            # Make sure exposure times overlap
            start_time, stop_time = expos0.synchronous_times(expos1)

            if not glob.glob(f'*{expos0.expid}+{expos1.expid}_RGS_rates.ds'): #If the lightcurves haven't already been generated, run rgslccorr
                
                logging.info(f"Running rgslccorr SAS command for observation number {self.obsid} and exposures {expos0.expid}, {expos1.expid} ...")
                rgslc_command = f"rgslccorr evlist='{expos0.evenli} {expos1.evenli}' srclist='{expos0.srcli} {expos1.srcli}' withbkgsubtraction=yes timebinsize=1000 timemin={start_time} timemax={stop_time} orders='1' sourceid=3 outputsrcfilename={self.obsid}_{expos0.expid}+{expos1.expid}_RGS_rates.ds outputbkgfilename={self.obsid}_{expos0.expid}+{expos1.expid}_bkg_rates.ds"
                status_rgslc = run_command(rgslc_command)
            
                #If an error occurred try running on separate exposures rgslccorr
                if status_rgslc!=0:
                    print(f'\033[91m An error has occurred running rgslccorr for observation {self.obsid}! \033[0m')
                
                #If no errors occurred, print to stdio success message
                else:
                    logging.info(f'RGS lightcurves successfully extracted.')
                
            else:
                logging.info(f'Lightcurves already extracted.')
            

    def lightcurve(self, mjdref, use_grace=False):
        """
        Makes the lightcurve plot and saves it in the rgs directory of the current observation
        The user here has two options: you can either save the lightcurve using the dsplot command
        with XmGrace as plotting package in '.ps' format, or you can plot it using python's
        module matplotlib and save it in the '.png' format. 
        You can choose the desired option setting the USE_GRACE boolean in the config.json file.

        :param mjdref: MJD corresponding to the beginning of the XMM-Newton mission. It is needed because the times are all in MET (Mission Elapsed Time)
        :type mjdref: float
        :param use_grace: if set to True, plots the lightcurve using the lotting interface Grace, if set to False it uses matplotlib
        :type use_grace: boolean
        """
        os.chdir(self.rgsdir)

        for filename in glob.glob('*_RGS_rates.ds'):
            output_name = filename
            try:
                if use_grace: 
                    logging.info(f'The RGS lightcurve {output_name} will be plotted with xmgrace.')
                    plot_lc_command = f'dsplot table={output_name} withx=yes x=TIME withy=yes y=RATE plotter="xmgrace -hardcopy -printfile {output_name}.ps"'
                    plot_status = run_command(plot_lc_command)
                    if (plot_status!=0):
                        raise Exception
                    else:
                        logging.info(f'Lightcurve {output_name} ready and saved.')

                else:  
                    logging.info(f'The lightcurve {output_name} will be plotted with matplotlib.')

                    #Extract data from the lightcurve fits file produced with rgslccorr and drop NaN values by making a numpy mask
                    x, y, yerr, fracexp, y_bg, yerr_bg = mask_fracexp15(output_name)
                
                    #Store average rate into Observation attribute
                    self.rgsrate.append(np.mean(y))
                    avg_rate = np.mean(y)
                    stdev_rate = np.sqrt(1/(np.sum(1/np.square(yerr))))  #weighted error of mean
                    self.stdev.append(stdev_rate)
                    avg_time = np.mean((x[0], x[-1]))
                    self.duration_lc_ks.append((x[-1] - x[0])/1000.)

                    #Conversion in MJD (note that 86400 are the seconds in one day)
                    avg_time_mjd = mjdref + (avg_time/86400.0)
                    self.longterm_lc_times.append(avg_time_mjd)

                    #Plot data: 1 panel for source lc, one panel for background lc
                    fig, axs = plt.subplots(2, 1, figsize=(15,10), sharex=True, gridspec_kw={'hspace':0})
                    axs[0].errorbar(x, y, yerr=yerr, color='black', fmt='.', elinewidth=1, capsize=2, capthick=1, markersize=5, linestyle='-', ecolor='gray', label=f'RGS Lightcurve ObsId {self.obsid}, exposures {output_name[11:18]} ')
                    axs[0].grid(True)
                    axs[0].set_ylabel('Rate [ct/s]', fontsize=13)
                    axs[0].hlines(avg_rate, plt.xlim()[0], plt.xlim()[1], colors='b', label=f'Average rate: {avg_rate: .2f} +- {stdev_rate:.2f} [ct/s]')
                    axs[0].tick_params(axis='both', which='major', labelsize=13)

                    axs[1].errorbar(x, y_bg, yerr=yerr_bg, color='red',fmt='.', elinewidth=1, capsize=2, capthick=1, markersize=5, linestyle='-', ecolor='rosybrown', label=f'Background')
                    axs[1].set_xlabel('Time [s]', fontsize=13)
                    axs[1].set_ylabel('Background Rate [ct/s]', fontsize=13)
                    axs[1].grid(True)
                    axs[1].tick_params(axis='both', which='major', labelsize=13)

                    #Magic trick for the legend
                    lines_labels = [ax.get_legend_handles_labels() for ax in fig.axes]
                    lines, labels = [sum(lol, []) for lol in zip(*lines_labels)]
                    fig.legend(lines, labels, loc='upper center', bbox_to_anchor=(0.5, 1.00), fontsize='x-large', fancybox=True, shadow=True)

                    #Save figure in rgs directory of the current Observation
                    plt.savefig(f'{output_name}.png')
                    plt.savefig(os.path.join(self.target_dir, "Products", "RGS_Lightcurves", f"{output_name}.png"))
                    plt.close()

                    logging.info(f'The lightcurve is saved as {output_name}.png')

            except Exception as e:
                logging.error(e)


    def epic_lightcurve(self):
        """
        Makes the lightcurve plots with matplotlib. The first plot will consist of 4 panels containing:
        the soft lightcurve (0.2 - 2 keV), the hard lightcurve (2 - 10 keV) and the respective background lightcurves.
        The second plot consists of 3 panels: the soft and hard lightcurves and the hardness ratio calculated as
        HR := (H-S)/(H+S) where H and S are the datapoints for the hard and soft lightcurves.
        """

        os.chdir(self.epdir)
        logging.info(f'Making soft and hard light curves for obs {self.obsid} with matplotlib.')

        #Extract data from the lightcurve fits file produced with epiclccorr and drop NaN values by making a numpy mask
        x_soft, y_soft, yerr_soft, fracexp_soft, y_bg_soft, yerr_bg_soft = mask_fracexp15('PN_soft.lc')
        x_hard, y_hard, yerr_hard, fracexp_hard, y_bg_hard, yerr_bg_hard = mask_fracexp15('PN_hard.lc')
        hr = (np.array(y_hard) - np.array(y_soft)) / (np.array(y_hard) + np.array(y_soft))
        hr_err = hr*(np.array(yerr_hard) + np.array(yerr_soft))* (1/(np.array(y_hard)-np.array(y_soft)) +1/(np.array(y_hard)+np.array(y_soft)))

        #Store average rate into Observation attribute
        self.epicrate.append(np.mean(y_soft))
        self.epicrate.append(np.mean(y_hard))
        avg_rate_soft = np.mean(y_soft)
        avg_rate_hard = np.mean(y_hard)
        erate_soft = np.sqrt(1/(np.sum(1/np.square(yerr_soft))))  #weighted error of mean
        erate_hard = np.sqrt(1/(np.sum(1/np.square(yerr_hard))))
        self.epic_erate.append(erate_soft)
        self.epic_erate.append(erate_hard)

        #Plot data: one panel for source lc, the other panel for background lc
        fig, axs = plt.subplots(2, 2, figsize=(12,8), sharex=True, gridspec_kw={'hspace':0.1, 'wspace':0.3})
        fig.suptitle(f'Soft and hard Light curves for Obs {self.obsid}')
        axs[0,0].errorbar(x_soft, y_soft, yerr=yerr_soft, color='black', fmt='.', elinewidth=1, capsize=2, capthick=1, markersize=5, linestyle='', ecolor='gray')
        axs[0,0].grid(True)
        axs[0,0].set_ylabel('Soft LC Rate [ct/s]', fontsize=13)
        #axs[0,0].hlines(avg_rate_soft, plt.xlim()[0], plt.xlim()[1], colors='b', label=f'Average rate: {avg_rate_soft: .2f} +- {erate_soft:.2f} [ct/s]')
        axs[0,0].tick_params(axis='both', which='major', labelsize=13)
        
        axs[1,0].errorbar(x_soft, y_bg_soft, yerr=yerr_bg_soft, color='red',fmt='.', elinewidth=1, capsize=2, capthick=1, markersize=5, linestyle='', ecolor='rosybrown')
        axs[1,0].set_xlabel('Time [s]', fontsize=13)
        axs[1,0].set_ylabel('Background Soft LC Rate [ct/s]', fontsize=13)
        axs[1,0].grid(True)
        axs[1,0].tick_params(axis='both', which='major', labelsize=13)

        axs[0,1].errorbar(x_hard, y_hard, yerr=yerr_hard, color='black', fmt='.', elinewidth=1, capsize=2, capthick=1, markersize=5, linestyle='', ecolor='gray')
        axs[0,1].grid(True)
        axs[0,1].set_ylabel('Hard LC Rate [ct/s]', fontsize=13)
        #axs[0,1].hlines(avg_rate_hard, plt.xlim()[0], plt.xlim()[1], colors='b', label=f'Average rate: {avg_rate_hard: .2f} +- {erate_hard:.2f} [ct/s]')
        axs[0,1].tick_params(axis='both', which='major', labelsize=13)

        axs[1,1].errorbar(x_hard, y_bg_hard, yerr=yerr_bg_hard, color='red',fmt='.', elinewidth=1, capsize=2, capthick=1, markersize=5, linestyle='', ecolor='rosybrown')
        axs[1,1].set_xlabel('Time [s]', fontsize=13)
        axs[1,1].set_ylabel('Hard LC Background Rate [ct/s]', fontsize=13)
        axs[1,1].grid(True)
        axs[1,1].tick_params(axis='both', which='major', labelsize=13)

        #Magic trick for the legend
        #lines_labels = [ax.get_legend_handles_labels() for ax in fig.axes]
        #lines, labels = [sum(lol, []) for lol in zip(*lines_labels)]
        #fig.legend(lines, labels, loc='upper center', bbox_to_anchor=(0.5, 1.00), fontsize='x-large', fancybox=True, shadow=True)

        #Save figure in epic/pn directory of the current Observation
        plt.savefig(f'{self.obsid}_epicpn_lc.png')
        if not os.path.isdir(f'{self.target_dir}/Products/EPIC_Lightcurves'):
            os.makedirs(f'{self.target_dir}/Products/EPIC_Lightcurves')

        plt.savefig(os.path.join(self.target_dir, "Products", "EPIC_Lightcurves", f"{self.obsid}_epicpn_lc.png"))
        plt.close()

        #Plot lightcurves with HR
        fig2, axs2 = plt.subplots(3, 1, figsize=(7,6), sharex=False, gridspec_kw={'hspace':0.0})
        axs2[0].errorbar(x_soft, y_soft, yerr=yerr_soft, color='black', fmt='.', elinewidth=1, capsize=2, capthick=1, markersize=5, linestyle='', ecolor='gray', label=f'EPIC-pn ObsId {self.obsid}, exposure {self.epic_expid} Soft LC')
        axs2[1].errorbar(x_hard, y_hard, yerr=yerr_hard, color='black', fmt='.', elinewidth=1, capsize=2, capthick=1, markersize=5, linestyle='', ecolor='gray', label=f'EPIC-pn ObsId {self.obsid}, exposure {self.epic_expid} Hard LC')
        axs2[2].errorbar(x_soft, hr, yerr=hr_err, color='black', fmt='.', elinewidth=1, capsize=2, capthick=1, markersize=5, linestyle='', ecolor='gray', label=f'EPIC-pn ObsId {self.obsid}, exposure {self.epic_expid} Hardness Ratio')
        axs2[2].set_xlabel('Time [s]')
        axs2[2].set_ylabel('HR: (H-S)/(H+S)')
        axs2[1].set_ylabel('Hard Light Curve')
        axs2[0].set_ylabel('Soft Light Curve')
        axs2[0].grid()
        axs2[1].grid()
        axs2[2].grid()
        #axs2[3].errorbar(y_soft+y_hard, hr, color='black', fmt='.', elinewidth=1, capsize=2, capthick=1, markersize=5, linestyle='', ecolor='gray' )

        plt.savefig(os.path.join(self.target_dir, "Products", "EPIC_Lightcurves", f"{self.obsid}_epicpn_HR.png"))
        plt.close()
        self.mean_hr = np.mean(hr)
        #obs_array = np.ndarray(len(y_soft))
        #obs_array.fill(str(self.obsid))
        table_hr_rate = Table({'rate': y_soft+y_hard, 'erate': yerr_soft+yerr_hard, 'hr': hr, 'hr_err': hr_err}, dtype=('d', 'd', 'd', 'd'))
        ascii.write(table=table_hr_rate, output=f'{self.target_dir}/Products/EPIC_Lightcurves/{self.obsid}_hr_rate.csv', format='csv', overwrite=True)

        logging.info(f'The lightcurve is saved as {self.obsid}_epicpn_lc.png')


    def bkg_lightcurve(self):
        """
        Generates Background lightcurve associated to each exposure. Useful to check if there are flares.
        Follows tutorial on SAS thread of RGS background.
        """
        logging.info('Generating Background lightcurve...')
        os.chdir(self.rgsdir)        

        flat_evenli = [item for sublist in self.pairs_events for item in sublist]
        flat_srcli = [item for sublist in self.pairs_srcli for item in sublist]
        evenli_srcli = list(zip(flat_evenli, flat_srcli))
       
        for (evenli, srcli) in evenli_srcli:
        
            expos0 = Exposure(evenli, srcli)
            title_outputbkg0 = f"bkg{expos0.fullid}_check_rates.fit"
            
            #Selects events from background region, CCD9 to search for flaring particles
            if not glob.glob(os.path.join(self.target_dir, "Products", "Backgrnd_LC", f"{title_outputbkg0}.png")):
                
                select_bkg_cmmd0 = f"evselect table={expos0.evenli} timebinsize=1000 rateset={title_outputbkg0} makeratecolumn=yes maketimecolumn=yes expression='(CCDNR==9)&&(REGION({expos0.srcli}:{expos0.instrume}_BACKGROUND, M_LAMBDA, XDSP_CORR))'"
                status_cmmd0 = run_command(select_bkg_cmmd0)

                #Retrieve data from selected eventlist of background
                data = fits.open(title_outputbkg0)
                x = data['RATE'].data['TIME']
                y = data['RATE'].data['RATE']               
                yerr = data['RATE'].data['ERROR']
                
                #Drop NaN values by making a numpy mask
                mask_nan = np.invert(np.isnan(y)) 
                x = x[mask_nan]
                y = y[mask_nan]
                yerr = yerr[mask_nan]
                mean_y = np.mean(y)

                #Plot data and add labels and title
                fig = plt.figure(figsize=(20,10))
                ax = fig.add_subplot(1, 1, 1)
                plt.errorbar(x, y, yerr=yerr, color='black', marker='.', ecolor='gray')
                plt.hlines(mean_y, plt.xlim()[0], plt.xlim()[1], colors='red')
                plt.grid(True)
                plt.title(title_outputbkg0, fontsize=30)
                plt.xlabel('TIME [s]', fontsize=25)
                plt.ylabel('RATE [count/s]', fontsize=25)
                plt.xticks(fontsize=20)
                plt.yticks(fontsize=20)

                #Save figure in rgs directory of the current Observation
                plt.savefig(os.path.join(self.target_dir, "Products", "Backgrnd_LC", f"{title_outputbkg0}.png"))
                plt.close()
                logging.info(f'Done generating background lightcurve for exposure {expos0.expid}!')
            
            else:
                logging.info(f"Already generated background lightcurve for exposure {expos0.expid}.")


    def check_flaring_particle_bkgr(self):
        """
        Checks if the background lightcurve (CCD9 of RGS) affected by solar flares ecc, has significant peaks. 
        In order to do this, the method calculates the mean of the background lightcurve, its standard deviation, 
        and if there are datapoints that are >3 sigma from the average, it counts that value as a significant flare. 
        The method collects all these flares in an array and then calls the SAS command tabgtigen and cuts on
        RATE<maxr where maxr is the minimum element of the flare array.
        """

        os.chdir(self.rgsdir)

        #Pair events and source regions
        flat_evenli = [item for sublist in self.pairs_events for item in sublist]
        flat_srcli = [item for sublist in self.pairs_srcli for item in sublist]
        evenli_srcli = list(zip(flat_evenli, flat_srcli))
        if len(evenli_srcli)==0:
            return
        
        flares = {}

        for (evenli, srcli) in evenli_srcli:
            
            expos0 = Exposure(evenli, srcli)
            title_outputbkg0 = f"bkg{expos0.fullid}_check_rates.fit"
            logging.info(f'Checking the flaring particle background for exposure {expos0.expid}.')
        
            #Retrieve data from flaring particle background
            data = fits.open(title_outputbkg0)
            x = data['RATE'].data['TIME']
            y = data['RATE'].data['RATE']               
            yerr = data['RATE'].data['ERROR']
            mean_y = np.mean(y)
            std_y = np.std(y)

            #Drop NaN values by making a numpy mask
            mask_nan = np.invert(np.isnan(y)) 
            x = x[mask_nan]
            y = y[mask_nan]
            yerr = yerr[mask_nan]

            #Search for significant flares
            for rate_value in y:
                
                if rate_value > (3*std_y + mean_y):
                    logging.info('Found flare in background!')
                    if title_outputbkg0 in flares:
                        if rate_value < flares[title_outputbkg0]:
                            logging.info('Updating flare dictionary.')
                            flares[title_outputbkg0] = rate_value
                        else:
                            continue
                    else:
                        flares[title_outputbkg0] = rate_value
            
        #If there are flares, filter the eventlist for the source lightcurve
        if len(flares)>0:

            max_key = min(flares, key=flares.get)
            maxr = flares[max_key]
            logging.info(f"Background lightcurves present significant flares in {max_key}. Starting the selection of the GTI...")
            
            if not glob.glob(os.path.join(self.rgsdir, f"gti_low_back_{max_key[13:16]}.fit")):
                tabgtigen_back_cmmd = f"tabgtigen table={max_key} gtiset=gti_low_back_{max_key[13:16]}.fit expression='(RATE<{maxr})'"
                status_cmmd = run_command(tabgtigen_back_cmmd)
                
                logging.info("Running rgsfilter...")
                merged_set = glob.glob(f"*merged0000.FIT")
                merged_set = sort_rgs_list(merged_set, 'expo_number')
                evenli_set = glob.glob(f"*EVENLI0000.FIT")
                evenli_set = sort_rgs_list(evenli_set, 'expo_number')
                merged_set = [item for sublist in merged_set for item in sublist]
                evenli_set = [item for sublist in evenli_set for item in sublist]    
                print(merged_set)
                print(evenli_set)   

                for i in range(len(merged_set)):
                    rgsfilter_cmmd = f"rgsfilter mergedset={merged_set[i]} evlist={evenli_set[i]} auxgtitables=gti_low_back_{max_key[13:16]}.fit"
                    status_rgsfilter = run_command(rgsfilter_cmmd)

                logging.info("Running rgsproc from spectra stage...")
                rgsproc_cmmd = "rgsproc entrystage=4:spectra"
                status_rgsproc = run_command(rgsproc_cmmd)
                logging.info("Finished rgsproc!")
            else:
                logging.info('Already filtered background.')

        else:
            logging.info(f"Background lightcurves present no significant flares. No need in filtering.")


    def filter_epic(self, pileup=False):
        """
        Filter an EPIC PN event list for periods of high background flaring activity.
        """
        self.epic_timebinsize = 200 #s from 100 to 500
        
        #PN data
        logging.info('Starting filtering from background flaring activity for EPIC-PN..')
        os.chdir(self.epdir)
        
        epic_pn_event = glob.glob(os.path.join(self.epdir, "*TimingEvts.ds"))[0]
  
        if not glob.glob(os.path.join(self.epdir, 'PNclean.fits')):  #58:77
            evselct_rate_pn = f"evselect table={epic_pn_event} withrateset=Y rateset=PNrate.fits maketimecolumn=Y timebinsize={self.epic_timebinsize} makeratecolumn=Y expression=' #XMMEA_EP && (PI>10000&&PI<12000) && (PATTERN==0)'"
            status_evselect_rate_pn = run_command(evselct_rate_pn)

            tabgtigen_pn = f"tabgtigen table=PNrate.fits expression='RATE<=0.4' gtiset=PNgti.fits"
            status_tabgtigen_pn = run_command(tabgtigen_pn)

            evselect_clean_pn = f"evselect table={epic_pn_event} withfilteredset=Y filteredset=PNclean.fits destruct=Y keepfilteroutput=T expression='#XMMEA_EP && gti(PNgti.fits,TIME) && (PI>150)'"
            status_evselect_clean_pn = run_command(evselect_clean_pn)

        else:
            logging.info(f"Already filtered EPIC-PN. The clean product is PNclean.fits.")
        
        if pileup:
            if not glob.glob('PNclean_noBore.fits'):
                try:
                    with open("ds9.reg") as f:
                        region = f.read()
                except FileNotFoundError as e:
                    print(e)
                    sys.exit(0)
            
                region = region.split('\n') #divide text file into lines

                #Source coordinates
                coordinates = region[3][4:-1].split(',')
                xcenter = float(coordinates[0])
                xmax = xcenter + float(coordinates[2])/2
                xmin = xcenter - float(coordinates[2])/2

                logging.info("Correcting for pile-up...")
                evselect_pileup = f"evselect table=PNclean.fits withfilteredset=yes filteredset=PNclean_noBore.fits keepfilteroutput=yes expression='(RAWX in [{xmin}:{xmax}]) &&! (RAWX in [{xcenter-1}:{xcenter+1}]) && gti(PNgti.fits,TIME)'"
                status_evselect_pileup = run_command(evselect_pileup)  
            else:
                logging.info('Already corrected for pile-up.')
    
        logging.info('Done filtering EPIC-PN!')


    def fracvartest(self, screen=True, netlightcurve=True, instrument='rgs'):
        """
        Reads the FITS file containing the RGS source and background timeseries produced by rgslccorr. 
        It then calculates excess variance, normalized excess variance and fractional variability of the lightcurve,
        storing all these values into a dictionary that will then be reported in the final .csv file.

        :param screen: if True, prints on terminal the results of the variability quantities, defaults to True
        :type screen: boolean
        :param netlightcurve: if True, uses the net lightcurve (i.e. background substracted) to calculate the variability quantities, defaults to True
        :type netlightcurve: boolean

        """
        if instrument=='rgs':
            os.chdir(self.rgsdir)

            #Recover all lightcurves included in the rgs Observation directory
            i = 0 
            for filename in glob.glob('*_RGS_rates.ds'):
                timeseries = filename
                
                try:
                    #Get dataset(s) and table.
                    with fits.open(timeseries) as hdul:
                        header = hdul['RATE'].header
                        astro_table = Table(hdul['RATE'].data)
                        dataset = astro_table.to_pandas()
                        dataset = dataset.sort_values(by=['TIME'])

                    #Check important keyword consistency 
                    if 'TIMEDEL' in header:
                        if header['TIMEDEL']<=0:
                            raise ValueError('\033[91m Null or negative Bin Width in Input File. This should be a positive value. \033[0m')
                    else:
                        raise IOError('\033[91m The TIMEDEL keyword is missing in the timeseries FITS file, which is necessary to determine the binning factor of the data. \033[0m')

                    if 'TSTART' not in header:
                        raise IOError('\033[91m Keyword TSTART missing in Input. There could be a problem with the input timeseries. \033[0m')
                    
                    if 'HDUCLASS' in header:
                        if not header['HDUCLASS']=='OGIP':
                            raise ValueError('\033[91m HDUCLASS is not equal to OGIP. \033[0m')
                    else:
                        raise IOError('\033[91m Keyword HDUCLASS missing in Input File. There could be a problem with the input FITS timeseries. \033[0m')

                    if 'HDUCLAS1' in header:
                        if not header['HDUCLAS1']=='LIGHTCURVE':
                            raise ValueError('\033[91m HDUCLAS1 is not equal to LIGHTCURVE. \033[0m')
                    else:
                        raise IOError('\033[91m Keyword HDUCLAS1 missing in Input File. There could be a problem with the input FITS timeseries. \033[0m')
                    
                except Exception as e:
                    logging.error(e)

                try:
                    #Delete gaps in data 
                    dataset_cleaned = dataset.dropna()

                    #Net source rates and errors and background rates and errors are recorded in arrays
                    numnonnull = len(dataset_cleaned)
                    rates = np.array(dataset_cleaned['RATE'])
                    errrates = np.array(dataset_cleaned['ERROR'])
                    backv = np.array(dataset_cleaned['BACKV'])
                    backe = np.array(dataset_cleaned['BACKE'])
                    time = np.array(dataset_cleaned['TIME'])

                    #Sanity checks
                    if numnonnull<2:
                        raise NoDataException('\033[91m Less than two good values in the timeseries FITS file.\033[0m')
                    if not len(rates)==len(errrates) and len(rates)==len(backv) and len(rates)==len(backe):
                        raise RangeException('\033[91m Different number of rows in columns between RATE, ERROR, BACKV and BACKE. \033[0m')
                    if not (rates>0).all() and (backv>0).all():
                        raise ValueError('\033[91m Negative count rates in Input File. \033[0m')

                except NoDataException as e:
                    logging.error(e)   
                except RangeException as e:
                    logging.error(e)
                except ValueError as e:
                    logging.error(e)

                #Calculate Variability parameters and respective errors
                xs, err_xs = excess_variance(rates, errrates, normalized=False)
                nxs, err_nxs = excess_variance(rates, errrates, normalized=True)
                f_var, err_fvar = fractional_variability(rates, errrates, backv, backe, netlightcurve=netlightcurve)
                
                va = (max(rates) - min(rates))/ (min(rates))
                err_va = va* ( (errrates[rates.argmax()] + errrates[rates.argmin()])/(max(rates) - min(rates)) +  (errrates[rates.argmin()] / min(rates) ))

                logging.info(f'Do you want to carry out the fractional varability amplitude test on the net lightcurve? {netlightcurve}.')
                self.fracvardict.append({"Excess variance": xs, "Excess variance error": err_xs, "Normalized excess variance": nxs,
                                    "Normalized excess variance error": err_nxs, "Fractional Variability": f_var, 
                                    "Fractional Variability Error": err_fvar,
                                    "Variability Amplitude": va, "Variability amplitude error": err_va,
                                    "Number of non null data points": numnonnull})
                
                #Write variability test results into header and/or to screen.
                if screen:
                    for key, value in self.fracvardict[i].items():
                        print(key, ' : ', value)
                i+=1

        elif instrument=='epic':
            os.chdir(self.epdir)

            #Recover all lightcurves included in the epic/pn Observation directory
            i = 0
            for filename in ['PN_soft.lc', 'PN_hard.lc']:
                timeseries = filename

                try:
                    #Get dataset(s) and table.
                    with fits.open(timeseries) as hdul:
                        header = hdul['RATE'].header
                        astro_table = Table(hdul['RATE'].data)
                        dataset = astro_table.to_pandas()
                        dataset = dataset.sort_values(by=['TIME'])

                    #Check important keyword consistency 
                    if 'TIMEDEL' in header:
                        if header['TIMEDEL']<=0:
                            raise ValueError('\033[91m Null or negative Bin Width in Input File. This should be a positive value. \033[0m')
                    else:
                        raise IOError('\033[91m The TIMEDEL keyword is missing in the timeseries FITS file, which is necessary to determine the binning factor of the data. \033[0m')

                    if 'TSTART' not in header:
                        raise IOError('\033[91m Keyword TSTART missing in Input. There could be a problem with the input timeseries. \033[0m')
                    
                    if 'HDUCLASS' in header:
                        if not header['HDUCLASS']=='OGIP':
                            raise ValueError('\033[91m HDUCLASS is not equal to OGIP. \033[0m')
                    else:
                        raise IOError('\033[91m Keyword HDUCLASS missing in Input File. There could be a problem with the input FITS timeseries. \033[0m')

                    if 'HDUCLAS1' in header:
                        if not header['HDUCLAS1']=='LIGHTCURVE':
                            raise ValueError('\033[91m HDUCLAS1 is not equal to LIGHTCURVE. \033[0m')
                    else:
                        raise IOError('\033[91m Keyword HDUCLAS1 missing in Input File. There could be a problem with the input FITS timeseries. \033[0m')
                    
                except Exception as e:
                    logging.error(e)

                try:
                    #Delete gaps in data 
                    dataset_cleaned = dataset.dropna()

                    #Net source rates and errors and background rates and errors are recorded in arrays
                    numnonnull = len(dataset_cleaned)
                    rates = np.array(dataset_cleaned['RATE'])
                    errrates = np.array(dataset_cleaned['ERROR'])
                    backv = np.array(dataset_cleaned['BACKV'])
                    backe = np.array(dataset_cleaned['BACKE'])
                    time = np.array(dataset_cleaned['TIME'])

                    #Sanity checks
                    if numnonnull<2:
                        raise NoDataException('\033[91m Less than two good values in the timeseries FITS file.\033[0m')
                    if not len(rates)==len(errrates) and len(rates)==len(backv) and len(rates)==len(backe):
                        raise RangeException('\033[91m Different number of rows in columns between RATE, ERROR, BACKV and BACKE. \033[0m')
                    if not (rates>0).all() and (backv>0).all():
                        raise ValueError('\033[91m Negative count rates in Input File. \033[0m')

                except NoDataException as e:
                    logging.error(e)   
                except RangeException as e:
                    logging.error(e)
                except ValueError as e:
                    logging.error(e)

                #Calculate Variability parameters and respective errors
                xs, err_xs = excess_variance(rates, errrates, normalized=False)
                nxs, err_nxs = excess_variance(rates, errrates, normalized=True)
                f_var, err_fvar = fractional_variability(rates, errrates, backv, backe, netlightcurve=netlightcurve)
                va = (max(rates) - min(rates))/ (min(rates))
                err_va = va* ( (errrates[rates.argmax()] + errrates[rates.argmin()])/(max(rates) - min(rates)) +  (errrates[rates.argmin()] / min(rates) ))

                logging.info(f'Do you want to carry out the fractional varability amplitude test on the net lightcurve? {netlightcurve}.')
                self.fracvardict.append({"Excess variance": xs, "Excess variance error": err_xs, "Normalized excess variance": nxs,
                                    "Normalized excess variance error": err_nxs, "Fractional Variability": f_var, 
                                    "Fractional Variability Error": err_fvar,
                                    "Variability Amplitude": va, "Variability amplitude error": err_va,
                                    "Number of non null data points": numnonnull})
                
                #Write variability test results into header and/or to screen.
                if screen:
                    for key, value in self.fracvardict[i].items():
                        print(key, ' : ', value)
                i+=1


    def pn_spectrum(self, pileup=False):
        """
        Follows https://www.cosmos.esa.int/web/xmm-newton/sas-thread-pn-spectrum-timing
        """
        os.chdir(self.epdir)

        if not glob.glob('PNsource_spectrum.fits'):

            #Open ds9 file where the coordinates of the source and background region are stored
            try:

                with open("ds9.reg") as f:
                    region = f.read()

            except FileNotFoundError as e:
                print(e)
                sys.exit(0)
                
            region = region.split('\n') #divide text file into lines

            #Source coordinates
            coordinates = region[3][4:-1].split(',')
            xcenter = float(coordinates[0])
            xmax = xcenter + float(coordinates[2])/2
            xmin = xcenter - float(coordinates[2])/2

            #Background coordinates
            coordinates_bkg = region[4][4:-1].split(',')
            xcenter_bkg = float(coordinates_bkg[0])
            xmax_bkg = xcenter_bkg + float(coordinates_bkg[2])/2
            xmin_bkg = xcenter_bkg - float(coordinates_bkg[2])/2

            #Extract a source spectrum
            logging.info("Extracting source spectrum...")
            if pileup:
                evselect_src_cmmd = f"evselect table=PNclean.fits withspectrumset=yes spectrumset=PNsource_spectrum.fits energycolumn=PI spectralbinsize=5 withspecranges=yes specchannelmin=0 specchannelmax=20479 expression='(FLAG==0) && (PATTERN<=4) && (RAWX>={xmin}) && (RAWX<={xmax}) &&! (RAWX in [{xcenter-1}:{xcenter+1}])'"
            else:
                evselect_src_cmmd = f"evselect table=PNclean.fits withspectrumset=yes spectrumset=PNsource_spectrum.fits energycolumn=PI spectralbinsize=5 withspecranges=yes specchannelmin=0 specchannelmax=20479 expression='(FLAG==0) && (PATTERN<=4) && (RAWX>={xmin}) && (RAWX<={xmax})'"

            status_evselect_src = run_command(evselect_src_cmmd)

            #Extract a background spectrum
            logging.info("Extracting background spectrum...")
            evselect_bkg_cmmd = f"evselect table=PNclean.fits withspectrumset=yes spectrumset=PNbackground_spectrum.fits \
                                energycolumn=PI spectralbinsize=5 withspecranges=yes specchannelmin=0 specchannelmax=20479 \
                                expression='(FLAG==0) && (PATTERN<=4) && (RAWX>={xmin_bkg}) && (RAWX<={xmax_bkg})'"
            status_evselect_bkg = run_command(evselect_bkg_cmmd)

            #Calculate the area of source and background region used to make the spectral files
            logging.info('Calculating the area of source and background region used to make the spectral files...')
            backscale_cmmd = "backscale spectrumset=PNsource_spectrum.fits badpixlocation=PNclean.fits"
            status_backscale_cmmd = run_command(backscale_cmmd)
    
            backscale_bkg_cmmd = "backscale spectrumset=PNbackground_spectrum.fits badpixlocation=PNclean.fits"
            status_backscale_bkg_cmmd = run_command(backscale_bkg_cmmd)

            #Generate a redistribution matrix
            logging.info('Generating a redistribution matrix...')
            rmf_cmmd = "rmfgen spectrumset=PNsource_spectrum.fits rmfset=PN.rmf"
            status_rmf = run_command(rmf_cmmd)

            #Generate an ancillary file 
            logging.info('Generating an ancillary file...')
            arf_cmmd = "arfgen spectrumset=PNsource_spectrum.fits arfset=PN.arf withrmfset=yes rmfset=PN.rmf badpixlocation=PNclean.fits detmaptype=psf"
            status_arf = run_command(arf_cmmd)

            #Rebin the spectrum
            logging.info('Rebinning the spectrum...')
            spec_grp_cmmd = "specgroup spectrumset=PNsource_spectrum.fits mincounts=25 oversample=3 rmfset=PN.rmf arfset=PN.arf backgndset=PNbackground_spectrum.fits groupedset=PN_spectrum_grp.fits"
            status_grp = run_command(spec_grp_cmmd)
        
        else:
            logging.info('PN spectrum already extracted!')


    def pn_xspec(self, target_REDSHIFT):
        """
        """

        # Make sure output directory exists
        if not os.path.isdir(f'{self.target_dir}/Products/EPIC_Spectra'):
            os.mkdir(f'{self.target_dir}/Products/EPIC_Spectra')
        if not glob.glob(f'{self.target_dir}/Products/EPIC_Spectra/average_{self.obsid}_*'):          
            logging.info(f"Starting EPIC spectral analysis with XSPEC for observation {self.obsid} (total, average spectra).")
            os.chdir(f"{self.target_dir}/{self.obsid}/epic/pn")

            #Set XSPEC verbosity and create xspec log file
            xspec.Xset.chatter = 10
            xspec.Xset.logChatter = 10
            logFile = xspec.Xset.openLog(f"{self.target_dir}/Products/EPIC_Spectra//XSPECLogFile_{self.obsid}_spectrum.txt") 
            logFile = xspec.Xset.log

            # Create Table that we will fill with the output (parameters, flux, luminosity...)
            self.epic_spectra_table = Table(names=('obsid', 'instrid', 'exposures_id',
                                        'exposure', 'model',
                                        'nH', 'nH_low', 'nH_up', 'nH_low68', 'nH_up68', 
                                        'phoindex', 'phoindex_low', 'phoindex_up', 'phoindex_low68', 'phoindex_up68',
                                        'beta', 'beta_low', 'beta_up', 'beta_low68', 'beta_up68', 
                                        'norm', 'norm_low', 'norm_up', 'norm_low68', 'norm_up68',
                                        'chi2', 'pchi2', 'dof', 
                                        'src_cts', 'esrc_cts', 'bkg_cts', 'ebkg_cts',
                                        'rate', 'erate', 
                                        'flux', 'flux_up', 'flux_low',
                                        'lumin', 'lumin_up', 'lumin_low'
                                        ),
                                        dtype=('i','U9', 'U9',
                                            'd','U20',
                                            'd','d', 'd','d', 'd',
                                            'd','d', 'd','d', 'd',
                                            'd','d','d','d', 'd', 
                                            'd','d','d','d', 'd', 
                                            'd','d','i', 
                                            'd', 'd', 'd', 'd',
                                            'd', 'd',
                                            'd', 'd', 'd', 
                                            'd', 'd', 'd'))

            
            # Xspec models we want to use for fitting
            model_list = ['const*tbabs*zlogpar', 'const*tbabs*zpowerlw']  

            #Load epic pn data                
            xspec.AllData(f"PN_spectrum_grp.fits")
            spectrum = xspec.AllData(1)

            # Ignore bad channels
            xspec.AllData.ignore("bad")
            xspec.AllData.ignore('**-0.6')

            for model in model_list:
                m1 = xspec.Model(model)

                if m1.expression=='constant*TBabs*zlogpar':
                    m1.setPars("1.0 -1", {6:0.0308})
                if m1.expression=='constant*TBabs*zpowerlw':
                    m1.setPars("1.0 -1", {4:0.0308})

                xspec.AllModels.show()

                #Perform Fit using chi2 as statistic for parameter estimation 
                xspec.Fit.statTest = "pchi"
                xspec.Fit.renorm()    #renormalize model to minimize statistic with current parameters
                xspec.Fit.query = 'yes'
                xspec.Fit.perform() 

                #Error calculation (confidence intervals 3 sigma, 90% confidence)
                if m1.expression=='constant*TBabs*zlogpar':
                    xspec.Fit.error("stopat 1000,, maximum 1000.0 2.706 2,3,4,7")   

                if m1.expression=='constant*TBabs*zpowerlw':
                    xspec.Fit.error("stopat 1000,, maximum 1000.0 2.706 2,3,5")

                #Plot
                epic_spectrum_plot_xspec(self.obsid, self.epic_expid, model, self.target_dir)

                #Calculate Flux and Luminosity and store their values 
                xspec.AllModels.calcFlux('0.2 10 err 100 90') #add limits!
                xspec.AllModels.calcLumin(f'0.2 10 {target_REDSHIFT} err') 
                flux = spectrum.flux[0] #erg/cm2/s
                lumin = spectrum.lumin[0] #e+44 erg/s
                flux_up = spectrum.flux[2]
                flux_low = spectrum.flux[1]
                lumin_up = spectrum.lumin[2]
                lumin_low = spectrum.lumin[1]

                #Store parameter results of fit
                if m1.expression=='constant*TBabs*zpowerlw':
                    nH = m1(2).values[0]
                    phoindex = m1(3).values[0]
                    norm = m1(5).values[0]

                    #Confidence intervals 90%
                    nH_low = m1(2).error[0]
                    nH_up = m1(2).error[1]
                    phoindex_low = m1(3).error[0]
                    phoindex_up = m1(3).error[1]
                    norm_low = m1(5).error[0]
                    norm_up = m1(5).error[1]
                    beta, beta_up, beta_low = (np.nan, np.nan, np.nan)

                    #Confidence intervals 68%
                    xspec.Fit.error("stopat 1000,, maximum 1000.0 1.0 2,3,5")
                    nH_low68 = m1(2).error[0]
                    nH_up68 = m1(2).error[1]
                    phoindex_low68 = m1(3).error[0]
                    phoindex_up68 = m1(3).error[1]
                    norm_low68 = m1(5).error[0]
                    norm_up68 = m1(5).error[1]
                    beta, beta_up68, beta_low68 = (np.nan, np.nan, np.nan)


                if m1.expression=='constant*TBabs*zlogpar':
                    nH = m1(2).values[0]
                    phoindex = m1(3).values[0]
                    beta = m1(4).values[0]
                    norm = m1(7).values[0]

                    #Confidence intervals 90%
                    nH_low = m1(2).error[0]
                    nH_up = m1(2).error[1]
                    phoindex_low = m1(3).error[0]
                    phoindex_up = m1(3).error[1]
                    beta_low = m1(4).error[0]
                    beta_up = m1(4).error[1]
                    norm_low = m1(7).error[0]
                    norm_up = m1(7).error[1]

                    #Confidence intervals 68%
                    xspec.Fit.error("stopat 1000,, maximum 1000.0 1.0 2,3,4,7") 
                    nH_low68 = m1(2).error[0]
                    nH_up68 = m1(2).error[1]
                    phoindex_low68 = m1(3).error[0]
                    phoindex_up68 = m1(3).error[1]
                    beta_low68 = m1(4).error[0]
                    beta_up68 = m1(4).error[1]
                    norm_low68 = m1(7).error[0]
                    norm_up68 = m1(7).error[1]

                fit_statistic = xspec.Fit.statistic
                test_statistic = xspec.Fit.testStatistic
                dof = xspec.Fit.dof

                # Retrieve rates and counts from xspec
                exposure_time = spectrum.exposure
                src_rate = spectrum.rate[0]  #net rate
                src_rate_std = spectrum.rate[1] 
                frac = spectrum.rate[2] #total rate

                src_cts = spectrum.exposure*spectrum.rate[0] 
                src_ects = spectrum.exposure*spectrum.rate[1]
                bkg_cts = (1. - frac/100) *src_cts
                bkg_ects = (1. - frac/100) *src_ects

                #Save output table
                self.epic_spectra_table.add_row((self.obsid, "pn", f"{self.epic_expid}",  exposure_time, m1.expression, nH, nH_low, nH_up,
                                    nH_low68, nH_up68, phoindex, phoindex_low, phoindex_up, phoindex_low68, phoindex_up68,
                                    beta, beta_low, beta_up, beta_low68, beta_up68, norm, norm_low, norm_up, norm_low68, norm_up68,
                                    fit_statistic, test_statistic,
                                    dof, src_cts, src_ects, bkg_cts, bkg_ects, src_rate, src_rate_std, 
                                    flux, flux_up, flux_low, lumin, lumin_up, lumin_low))

                # Close XSPEC's currently opened log file.
                xspec.Xset.closeLog()

                # Write FITS output file 
                if glob.glob(f'{self.target_dir}/Products/EPIC_Spectra/{self.obsid}_table.fits'):
                    os.remove(glob.glob(f'{self.target_dir}/Products/EPIC_Spectra/{self.obsid}_table.fits')[0])

                self.epic_spectra_table.write(f'{self.target_dir}/Products/EPIC_Spectra/{self.obsid}_table.fits', format='fits', overwrite=True)
                logging.info('Done average spectral analysis. Please check your Products directory.')
        else:
            logging.info('Spectral analysis on average spectrum already performed.')


    def divide_spectrum(self):
        """
        Splits the spectrum into pieces of 1000 seconds each. To do this, I use a while loop with 2 indices:
        - j represents the end_time of each piece, so after an iteration you just add 1000 seconds to it;
        - i represents the start_time of each piece, so after an iteration you just set it to j.
        At the end of the processing, we will have cut into pieces of 1000s each exposure. For instance, if there are 50 pieces for each exposure, the output will be 50 spectra for RGS1 and 50 spectra for RGS2. The nomenclature of the output spectra is the following:

        sourcespec{instrume}_gti{k}.fits for the source spectrum
        bkgspec{instrume}_gti{k}.fits for the background spectrum

        where instrume can be RGS1 or RGS2, and k=1,2,3...50 is the piece we are considering.
        """

        #Store the eventlists and sourcelists
        os.chdir(self.rgsdir)
        respli = glob.glob('*RSPMAT1*')
        total_spectra = glob.glob('*SRSPEC1*')
        total_bkgr = glob.glob('*BGSPEC1*')

        # Pair the events (RGS1+RGS2) and sort them according to the exposure number(first RGS1, second RGS2)
        self.pairs_respli = sort_rgs_list(respli, "expo_number")
        self.pairs_spectra = sort_rgs_list(total_spectra, "expo_number")
        self.pairs_bkg = sort_rgs_list(total_bkgr, "expo_number")


        if self.obsid=='0510610101':
            self.pairs_respli = [['P0510610101R1S004RSPMAT1003.FIT', 'P0510610101R2S005RSPMAT1003.FIT'], ['P0510610101R1S004RSPMAT1003.FIT', 'P0510610101R2S013RSPMAT1003.FIT']]
            self.pairs_spectra = [['P0510610101R1S004SRSPEC1003.FIT', 'P0510610101R2S005SRSPEC1003.FIT'], ['P0510610101R1S004SRSPEC1003.FIT', 'P0510610101R2S013SRSPEC1003.FIT']]
            self.pairs_bkg = [['P0510610101R1S004BGSPEC1003.FIT','P0510610101R2S005BGSPEC1003.FIT'], ['P0510610101R1S004BGSPEC1003.FIT', 'P0510610101R2S013BGSPEC1003.FIT']]

        if self.obsid=='0510610201':
            self.pairs_respli = [['P0510610201R1S004RSPMAT1003.FIT', 'P0510610201R2S005RSPMAT1003.FIT'], ['P0510610201R1S015RSPMAT1003.FIT', 'P0510610201R2S005RSPMAT1003.FIT']]
            self.pairs_spectra = [['P0510610201R1S004SRSPEC1003.FIT', 'P0510610201R2S005SRSPEC1003.FIT'], ['P0510610201R1S015SRSPEC1003.FIT', 'P0510610201R2S005SRSPEC1003.FIT']]
            self.pairs_bkg = [['P0510610201R1S004BGSPEC1003.FIT', 'P0510610201R2S005BGSPEC1003.FIT'], ['P0510610201R1S015BGSPEC1003.FIT', 'P0510610201R2S005BGSPEC1003.FIT']]

        if self.obsid=='0136540701':
            self.pairs_respli = [['P0136540701R1S001RSPMAT1003.FIT','P0136540701R2S002RSPMAT1003.FIT' ], ['P0136540701R1S001RSPMAT1003.FIT', 'P0136540701R2S018RSPMAT1003.FIT']]
            self.pairs_spectra = [['P0136540701R1S001SRSPEC1003.FIT','P0136540701R2S002SRSPEC1003.FIT' ], ['P0136540701R1S001SRSPEC1003.FIT', 'P0136540701R2S018SRSPEC1003.FIT']]
            self.pairs_bkg = [['P0136540701R1S001BGSPEC1003.FIT','P0136540701R2S002BGSPEC1003.FIT' ], ['P0136540701R1S001BGSPEC1003.FIT', 'P0136540701R2S018BGSPEC1003.FIT']]

        self.npairs = len(self.pairs_events)
        print("Event lists: ", self.pairs_events)
        print("Source lists: ", self.pairs_srcli)
        print("Response matrices: ", self.pairs_respli)
        print("Spectra: ", self.pairs_spectra)
        print("Bkgrounds: ", self.pairs_bkg)

        if not os.path.isdir(f'divided_spectra'):
            os.mkdir('divided_spectra')
        
        k = 1
        self.n_intervals_array =[]

        if not os.path.isdir(f'{self.target_dir}/Products/RGS_Spectra/{self.obsid}'):
            os.mkdir(f'{self.target_dir}/Products/RGS_Spectra/{self.obsid}')

        if not glob.glob(f'{self.target_dir}/Products/RGS_Spectra/{self.obsid}/*'):     

            #Check if the exposures are synchronous
            for l in range(self.npairs):  

                expos0 = Exposure(self.pairs_events[l][0], self.pairs_srcli[l][0], self.pairs_spectra[l][0])
                expos1 = Exposure(self.pairs_events[l][1], self.pairs_srcli[l][1], self.pairs_spectra[l][1])
                start_time, stop_time = expos0.synchronous_times(expos1)
                print(f"Synchronous start and stop times for exposures {expos0.evenli}, {expos1.evenli}: {start_time} - {stop_time}")

                #Divide each eventlist into pieces of 1000 seconds

                #if not glob.glob(f'divided_spectra/sourcespec{expos0.instrume}_{expos0.expid}_gti*') or not glob.glob(f'divided_spectra/sourcespec{expos1.instrume}_{expos1.expid}_gti*') :                
        
                print(f"Processing {expos0.evenli} and {expos1.evenli}...")

                # Initialize indices for loop
                step = 1000 #seconds
                i = start_time
                j = i + step
                if self.obsid not in ['0510610101', '0510610201', '0136540701']:
                    k = 1   #counter of pieces
                self.n_intervals_array.append(k)
                print(k)

                # Divide evenlist in parts of 1000s
                while (i<j) and (j<=stop_time):

                    # Cut and save each piece using SAS command tabgtigen and save output in rgs/divided_spectra/gti{instrume}_file{k}.fits
                    tabgtigen_cmd0 = f"tabgtigen table={expos0.evenli} expression='TIME in [{i}:{j}]' " \
                                    f"gtiset=divided_spectra/gti{expos0.instrume}_{expos0.expid}_file{k}.fits prefraction=0 postfraction=0" 
                    status_tabgtigen0 = run_command(tabgtigen_cmd0)
                    if status_tabgtigen0 !=0:    #Debug
                        print(f'Exception occured with piece n.{k}, exposure {expos0.evenli}, tabgtigen command')

                    tabgtigen_cmd1 = f"tabgtigen table={expos1.evenli} expression='TIME in [{i}:{j}]' " \
                                    f"gtiset=divided_spectra/gti{expos1.instrume}_{expos1.expid}_file{k}.fits prefraction=0 postfraction=0" 
                    status_tabgtigen1 = run_command(tabgtigen_cmd1)
                    if status_tabgtigen1 !=0:    #Debug
                        print(f'Exception occured with piece n.{k}, exposure {expos1.evenli}, tabgtigen command')


                    # Filter the piece that we just created
                    rgsfilter_cmd0 = f"rgsfilter mergedset={expos0.evenli} " \
                                    f"evlist=divided_spectra/event_gti{expos0.instrume}_{expos0.expid}_file{k}.fits " \
                                    f"auxgtitables=divided_spectra/gti{expos0.instrume}_{expos0.expid}_file{k}.fits" 
                    status_rgsfilter0 = run_command(rgsfilter_cmd0)
                    if status_rgsfilter0 !=0:    #Debug
                        print(f'Exception occured with piece n.{k}, exposure {expos0.evenli}, rgsfilter command')

                    rgsfilter_cmd1 = f"rgsfilter mergedset={expos1.evenli} " \
                                    f"evlist=divided_spectra/event_gti{expos1.instrume}_{expos1.expid}_file{k}.fits " \
                                    f"auxgtitables=divided_spectra/gti{expos1.instrume}_{expos1.expid}_file{k}.fits" 
                    status_rgsfilter1 = run_command(rgsfilter_cmd1)
                    if status_rgsfilter1 !=0:    #Debug
                        print(f'Exception occured with piece n.{k}, exposure {expos1.evenli}, rgsfilter command')

                    # Extract spectrum from the piece and save ouput as rgs/divided_spectra/sourcespec{instrume}_gti{k}.fits
                    rgsspectrum_cmd0 = f"rgsspectrum evlist=divided_spectra/event_gti{expos0.instrume}_{expos0.expid}_file{k}.fits " \
                                    f"srclist={expos0.srcli} withspectrum=yes bkgcorrect=no " \
                                    f"spectrumset=divided_spectra/sourcespec{expos0.instrume}_{expos0.expid}_gti{k}.fits withbkgset=yes " \
                                    f"bkgset=divided_spectra/bgspec{expos0.instrume}_{expos0.expid}_gti{k}.fits order=1 rebin=1 edgechannels=2 "\
                                    f"spectrumbinning=lambda withfracexp=no badquality=1"
                    status_rgsspectrum0 = run_command(rgsspectrum_cmd0)
                    if status_rgsspectrum0 !=0:    #Debug
                        print(f'Exception occured with piece n.{k}, exposure {expos0.evenli}, rgsspectrum command')
                    else:
                        print(f'Extracted piece number {k}, exposure {expos0.evenli}.')

                    rgsspectrum_cmd1 = f"rgsspectrum evlist=divided_spectra/event_gti{expos1.instrume}_{expos1.expid}_file{k}.fits " \
                                    f"srclist={expos1.srcli} withspectrum=yes bkgcorrect=no " \
                                    f"spectrumset=divided_spectra/sourcespec{expos1.instrume}_{expos1.expid}_gti{k}.fits withbkgset=yes " \
                                    f"bkgset=divided_spectra/bgspec{expos1.instrume}_{expos1.expid}_gti{k}.fits order=1 rebin=1 edgechannels=2 "\
                                    f"spectrumbinning=lambda withfracexp=no badquality=1"
                    status_rgsspectrum1 = run_command(rgsspectrum_cmd1)
                    if status_rgsspectrum1 !=0:    #Debug
                        print(f'Exception occured with piece n.{k}, exposure {expos1.evenli}, rgsspectrum command')
                    else:
                        print(f'Extracted piece number {k}, exposure {expos1.evenli}.')
                    
                    # Change index after each iteration
                    i = j
                    j = i + step
                    if j>stop_time:
                        j = stop_time
                    k += 1

                print(f"Done dividing {expos0.evenli} and {expos1.evenli}!") 
                print(k-1)
                self.n_intervals_array.append(k-1)         

            #else:
                #   print(f"Divided spectra already extracted for {expos0.evenli} and {expos1.evenli}.")
                    
                
    def xspec_divided_spectra_average(self, target_REDSHIFT):
        """
        XSPEC analysis of the total, average spectrum (RGS1+RGS2) of the observation. The steps are all logged into a file called XSPECLogFile_average_spectrum.txt.
        The fit is performed on two different models: logparabola and powerlaw. The plot of the spectra and residuals is done
        using matplotlib. The plotting function is written in tools.py
        The flux and luminosity are stored, given the target_REDSHIFT as argument.
        The fitted parameters and the spectrum counts are all stored into an astropy Table that is then saved as a FITS file.
        
        :param target_REDSHIFT: redshift of target
        :type target_REDSHIFT: float

        """    
        # Make sure output directory exists
        if not os.path.isdir(f'{self.target_dir}/Products/RGS_Spectra/{self.obsid}'):
            os.mkdir(f'{self.target_dir}/Products/RGS_Spectra/{self.obsid}')

        if not glob.glob(f'{self.target_dir}/Products/RGS_Spectra/{self.obsid}/*_1.png'):          
            logging.info(f"Starting RGS spectral analysis with XSPEC for observation {self.obsid} (total, average spectra).")
            os.chdir(f"{self.target_dir}/{self.obsid}/rgs")

            #Set XSPEC verbosity and create xspec log file
            xspec.Xset.chatter = 10
            xspec.Xset.logChatter = 10
            logFile = xspec.Xset.openLog(f"{self.target_dir}/Products/RGS_Spectra/{self.obsid}/XSPECLogFile_average_spectrum.txt") 
            logFile = xspec.Xset.log

            # Create Table that we will fill with the output (parameters, flux, luminosity...)
            self.spectra_table = Table(names=('obsid', 'instrid', 'exposures_id',
                                        'tbinid', 'tbinstart', 'tbinstop', 'exposure', 'model',
                                        'nH', 'nH_low', 'nH_up', 'nH_low68', 'nH_up68', 
                                        'phoindex', 'phoindex_low', 'phoindex_up', 'phoindex_low68', 'phoindex_up68',
                                        'beta', 'beta_low', 'beta_up', 'beta_low68', 'beta_up68', 
                                        'norm', 'norm_low', 'norm_up', 'norm_low68', 'norm_up68',
                                        'cstat', 'chi2', 'dof', 
                                        'src_cts', 'esrc_cts', 'bkg_cts', 'ebkg_cts',
                                        'rate', 'erate', 
                                        'flux', 'flux_up', 'flux_low',
                                        'lumin', 'lumin_up', 'lumin_low'
                                        ),
                                        dtype=('i','U9', 'U9',
                                            'i', 'd', 'd', 'd','U20',
                                            'd','d', 'd','d', 'd',
                                            'd','d', 'd','d', 'd',
                                            'd','d','d','d', 'd', 
                                            'd','d','d','d', 'd', 
                                            'd','d','i', 
                                            'd', 'd', 'd', 'd',
                                            'd', 'd',
                                            'd', 'd', 'd', 
                                            'd', 'd', 'd'))

            
            # Xspec models we want to use for fitting
            model_list = ['const*tbabs*zlogpar', 'const*tbabs*zpowerlw']  

            for i in range(self.npairs):
                expos0 = Exposure(self.pairs_events[i][0], self.pairs_srcli[i][0], self.pairs_spectra[i][0], self.pairs_bkg[i][0], self.pairs_respli[i][0])
                expos1 = Exposure(self.pairs_events[i][1], self.pairs_srcli[i][1], self.pairs_spectra[i][1], self.pairs_bkg[i][1], self.pairs_respli[i][1])
                start_time, stop_time = expos0.synchronous_times(expos1)

                #Load RGS1 + RGS2 data                
                os.chdir(f"{self.target_dir}/{self.obsid}/rgs")
                xspec.AllData(f"1:1 {expos0.specli} 2:2 {expos1.specli}")

                spectrum1 = xspec.AllData(1)
                spectrum1.background = f"{expos0.bkgli}"
                spectrum1.response = f"{expos0.respli}"

                spectrum2 = xspec.AllData(2)
                spectrum2.background = f"{expos1.bkgli}"
                spectrum2.response = f"{expos1.respli}"

                # Ignore bad channels
                xspec.AllData.ignore("bad")
                xspec.AllData.ignore('**-0.331 2.001-**')

                for model in model_list:
                    m1 = xspec.Model(model)
                    m2 = xspec.AllModels(2) #Retrieve the model object assigned to data group 2

                    if m1.expression=='constant*TBabs*zlogpar':
                        m1.setPars("1.0 -1", {5:0.331, 6:0.0308})
                        m2.setPars("1.0", "/*")

                    if m1.expression=='constant*TBabs*zpowerlw':
                        m1.setPars("1.0 -1", {4:0.0308})
                        m2.setPars("1.0", "/*")

                    xspec.AllModels.show()

                    #Perform Fit using cstat as statistic for parameter estimation
                    xspec.Fit.statMethod = "cstat" 
                    xspec.Fit.statTest = "pchi"
                    xspec.Fit.renorm()    #renormalize model to minimize statistic with current parameters
                    xspec.Fit.query = 'yes'
                    xspec.Fit.perform() 

                    #Error calculation (confidence intervals 3 sigma, 90% confidence)
                    if m1.expression=='constant*TBabs*zlogpar':
                        xspec.Fit.error("stopat 1000,, maximum 1000.0 2.706 2,3,4,7")   

                    if m1.expression=='constant*TBabs*zpowerlw':
                        xspec.Fit.error("stopat 1000,, maximum 1000.0 2.706 2,3,5")

                    #Plot
                    spectrum_plot_xspec(self.obsid, expos0.expid, expos1.expid, model, self.target_dir, 0)

                    #Calculate Flux and Luminosity and store their values 
                    xspec.AllModels.calcFlux('0.331 2.001 err 100 90')
                    xspec.AllModels.calcLumin(f'0.331 2.001 {target_REDSHIFT} err') 
                    flux = spectrum1.flux[0] #erg/cm2/s
                    lumin = spectrum1.lumin[0] #e+44 erg/s
                    flux_up = spectrum1.flux[2]
                    flux_low = spectrum1.flux[1]
                    lumin_up = spectrum1.lumin[2]
                    lumin_low = spectrum1.lumin[1]

                    #Store parameter results of fit
                    if m1.expression=='constant*TBabs*zpowerlw':
                        nH = m1(2).values[0]
                        phoindex = m1(3).values[0]
                        norm = m1(5).values[0]

                        #Confidence intervals 90%
                        nH_low = m1(2).error[0]
                        nH_up = m1(2).error[1]
                        phoindex_low = m1(3).error[0]
                        phoindex_up = m1(3).error[1]
                        norm_low = m1(5).error[0]
                        norm_up = m1(5).error[1]
                        beta, beta_up, beta_low = (np.nan, np.nan, np.nan)

                        #Confidence intervals 68%
                        xspec.Fit.error("stopat 1000,, maximum 1000.0 1.0 2,3,5")
                        nH_low68 = m1(2).error[0]
                        nH_up68 = m1(2).error[1]
                        phoindex_low68 = m1(3).error[0]
                        phoindex_up68 = m1(3).error[1]
                        norm_low68 = m1(5).error[0]
                        norm_up68 = m1(5).error[1]
                        beta, beta_up68, beta_low68 = (np.nan, np.nan, np.nan)


                    if m1.expression=='constant*TBabs*zlogpar':
                        nH = m1(2).values[0]
                        phoindex = m1(3).values[0]
                        beta = m1(4).values[0]
                        norm = m1(7).values[0]

                        #Confidence intervals 90%
                        nH_low = m1(2).error[0]
                        nH_up = m1(2).error[1]
                        phoindex_low = m1(3).error[0]
                        phoindex_up = m1(3).error[1]
                        beta_low = m1(4).error[0]
                        beta_up = m1(4).error[1]
                        norm_low = m1(7).error[0]
                        norm_up = m1(7).error[1]

                        #Confidence intervals 68%
                        xspec.Fit.error("stopat 1000,, maximum 1000.0 1.0 2,3,4,7") 
                        nH_low68 = m1(2).error[0]
                        nH_up68 = m1(2).error[1]
                        phoindex_low68 = m1(3).error[0]
                        phoindex_up68 = m1(3).error[1]
                        beta_low68 = m1(4).error[0]
                        beta_up68 = m1(4).error[1]
                        norm_low68 = m1(7).error[0]
                        norm_up68 = m1(7).error[1]

                    fit_statistic = xspec.Fit.statistic
                    test_statistic = xspec.Fit.testStatistic
                    dof = xspec.Fit.dof

                    # Retrieve rates and counts from xspec
                    exposure_time = max(spectrum1.exposure, spectrum2.exposure)
                    src_rate = spectrum1.rate[0] + spectrum2.rate[0]  #net rate
                    src_rate_std = spectrum1.rate[1] + spectrum2.rate[1]
                    frac = spectrum1.rate[2] + spectrum2.rate[2] #total rate

                    src_cts = spectrum1.exposure*spectrum1.rate[0] + spectrum2.exposure*spectrum2.rate[0]
                    src_ects = spectrum1.exposure*spectrum1.rate[1] + spectrum2.exposure*spectrum2.rate[1] 
                    bkg_cts = (1. - frac/100) *src_cts
                    bkg_ects = (1. - frac/100) *src_ects

                    #Save output table
                    self.spectra_table.add_row((self.obsid, "rgs12", f"{expos0.expid}+{expos1.expid}", 0, start_time, stop_time,  exposure_time, m1.expression, nH, nH_low, nH_up,
                                        nH_low68, nH_up68, phoindex, phoindex_low, phoindex_up, phoindex_low68, phoindex_up68,
                                        beta, beta_low, beta_up, beta_low68, beta_up68, norm, norm_low, norm_up, norm_low68, norm_up68,
                                        fit_statistic, test_statistic,
                                        dof, src_cts, src_ects, bkg_cts, bkg_ects, src_rate, src_rate_std, 
                                        flux, flux_up, flux_low, lumin, lumin_up, lumin_low))

                    # Set column units
                    self.spectra_table['tbinstart'].unit = 's'
                    self.spectra_table['tbinstop'].unit = 's'
                    self.spectra_table['exposure'].unit = 's'
                    self.spectra_table['nH'].unit = '10**(22) g / (cm2)'
                    self.spectra_table['nH_up'].unit = '10**(22) g/ (cm2)'
                    self.spectra_table['nH_low'].unit = '10**(22) g/ (cm2)'
                    self.spectra_table['norm'].unit = 'ph /(cm2 s)'
                    self.spectra_table['norm_low'].unit = 'ph/(cm2 s)'
                    self.spectra_table['norm_up'].unit = 'ph/(cm2 s)'
                    self.spectra_table['src_cts'].unit = 'ct'
                    self.spectra_table['bkg_cts'].unit = 'ct'
                    self.spectra_table['ebkg_cts'].unit = 'ct'
                    self.spectra_table['esrc_cts'].unit = 'ct'
                    self.spectra_table['rate'].unit = 'ct/s'
                    self.spectra_table['flux'].unit = 'erg/(cm2 s)'
                    self.spectra_table['flux_up'].unit = 'erg/(cm2 s)'
                    self.spectra_table['flux_low'].unit = 'erg/(cm2 s)'
                    self.spectra_table['lumin'].unit = '10**(44) erg /s'
                    self.spectra_table['lumin_up'].unit = '10**(44) erg/s'
                    self.spectra_table['lumin_low'].unit = '10**(44) erg/s'


            # Close XSPEC's currently opened log file.
            xspec.Xset.closeLog()

            # Write FITS output file 
            if glob.glob(f'{self.target_dir}/Products/RGS_Spectra/{self.obsid}/{self.obsid}_table.fits'):
                os.remove(glob.glob(f'{self.target_dir}/Products/RGS_Spectra/{self.obsid}/{self.obsid}_table.fits')[0])

            self.spectra_table.write(f'{self.target_dir}/Products/RGS_Spectra/{self.obsid}/{self.obsid}_table.fits', format='fits', overwrite=True)
            logging.info('Done average spectral analysis. Please check your Products directory.')
        else:
            logging.info('Spectral analysis on average spectrum already performed.')


    def xspec_divided_spectra(self, target_REDSHIFT):
        """
        If the divide_spectrum method has already been called, this method allows to perform analysis on all the pieces 
        into which we have divided the spectrum using a for loop. The results are stored in the same astropy Table
        of the average spectrum.

        :param target_REDSHIFT: redshift of target
        :type target_REDSHIFT: float
        
        """
        if not glob.glob(f'{self.target_dir}/Products/RGS_Spectra/{self.obsid}/*_1.png'):
                
            logging.info(f"Starting spectral analysis with XSPEC for observation {self.obsid}, split spectrum.")
            os.chdir(f"{self.target_dir}/{self.obsid}/rgs")

            #Set XSPEC verbosity and create xspec log file
            xspec.Xset.chatter = 10
            xspec.Xset.logChatter = 10
            logFile = xspec.Xset.openLog(f"{self._target_dir}/Products/RGS_Spectra/{self.obsid}/XSPECLogFile_divided_spectra.txt") 
            logFile = xspec.Xset.log

            for s in range(len(self.pairs_spectra)):
                expos0 = Exposure(self.pairs_events[s][0], self.pairs_srcli[s][0], self.pairs_spectra[s][0], self.pairs_bkg[s][0], self.pairs_respli[s][0])
                expos1 = Exposure(self.pairs_events[s][1], self.pairs_srcli[s][1], self.pairs_spectra[s][1], self.pairs_bkg[s][1], self.pairs_respli[s][1])
                start_time, stop_time = expos0.synchronous_times(expos1)

                # Perform spectral analysis looping over the pieces
                self.n_intervals_pairs = [self.n_intervals_array[x:x+2] for x in range(0, len(self.n_intervals_array), 2)]
                print(self.n_intervals_pairs)
                for i in range(self.n_intervals_pairs[s][0], self.n_intervals_pairs[s][1]):
        
                    logging.info(f'Processing gtiRGS1_{expos0.expid}_file{i}.fits for observation {self.obsid} ')
                    if not glob.glob(f'divided_spectra/gtiRGS1_{expos0.expid}_file{i}.fits'):
                        continue

                    try:
                        with fits.open(f'divided_spectra/gtiRGS1_{expos0.expid}_file{i}.fits') as hdul:
                            tstart = hdul['STDGTI'].header['TSTART']
                            tstop = hdul['STDGTI'].header['TSTOP']
                    except KeyError as e:
                        logging.error(e)
                        continue

                    #Load RGS1 + RGS2 data
                    xspec.AllData(f"1:1 divided_spectra/sourcespecRGS1_{expos0.expid}_gti{i}.fits 2:2 divided_spectra/sourcespecRGS2_{expos1.expid}_gti{i}.fits")

                    spectrum1 = xspec.AllData(1)
                    spectrum1.background = f"divided_spectra/bgspecRGS1_{expos0.expid}_gti{i}.fits"
                    spectrum1.response = f"{expos0.respli}"

                    spectrum2 = xspec.AllData(2)
                    spectrum2.background = f"divided_spectra/bgspecRGS2_{expos1.expid}_gti{i}.fits"
                    spectrum2.response = f"{expos1.respli}"

                    # Ignore bad channels
                    xspec.AllData.ignore("bad")
                    xspec.AllData.ignore('**-0.331 2.001-**')

                    #Calculate exposure time: if not at least 500 s, skip interval
                    exposure_time = max(spectrum1.exposure, spectrum2.exposure)
                    if exposure_time<500:
                        continue

                    #Loop over models
                    model_list = ['const*tbabs*zlogpar', 'const*tbabs*zpowerlw']
                    for model in model_list:
                        m1 = xspec.Model(model)
                        m2 = xspec.AllModels(2) #Retrieve the model object assigned to data group 2

                        if m1.expression=='constant*TBabs*zlogpar':
                            m1.setPars("1.0 -1", {5:0.331, 6:0.0308})
                            m2.setPars("1.0", "/*")

                        if m1.expression=='constant*TBabs*zpowerlw':
                            m1.setPars("1.0 -1", {4:0.0308})
                            m2.setPars("1.0", "/*")

                        #Perform Fit using cstat as statistic for parameter estimation
                        xspec.Fit.statMethod = "cstat" 
                        xspec.Fit.statTest = "pchi"
                        xspec.Fit.renorm()    #renormalize model to minimize statistic with current parameters
                        xspec.Fit.nIterations = 100
                        xspec.Fit.criticalDelta = 1e-1
                        xspec.Fit.query = 'no' 
                        
                        try:
                            xspec.Fit.perform() 
                        except Exception as e:
                            logging.error(e)
                            continue

                        #Error calculation (3 sigma confidence intervals)
                        try:
                            if m1.expression=='constant*TBabs*zlogpar':
                                xspec.Fit.error("stopat 1000,, maximum 1000.0 2.706 2,3,4,7")    

                            if m1.expression=='constant*TBabs*zpowerlw':
                                xspec.Fit.error("stopat 1000,, maximum 1000.0 2.706 2,3,5")

                        except Exception as e:
                            logging.error(e)
                            continue

                        #Plotting
                        spectrum_plot_xspec(self.obsid, expos0.expid, expos1.expid, model, self.target_dir, i)

                        #Calculate Flux and Luminosity and store their values (90% confidence intervals)
                        try:
                            xspec.AllModels.calcFlux('0.331 2.001 err 100 90')
                            xspec.AllModels.calcLumin(f'0.331 2.001 {target_REDSHIFT} err') 
                            flux = spectrum1.flux[0] #erg/cm2/s
                            lumin = spectrum1.lumin[0] #e+44 erg/s
                            flux_up = spectrum1.flux[2]
                            flux_low = spectrum1.flux[1]
                            lumin_up = spectrum1.lumin[2]
                            lumin_low = spectrum1.lumin[1]

                        except Exception as e:
                            logging.error(e)
                            continue

                        #Store parameter results of fit
                        if m1.expression=='constant*TBabs*zpowerlw':
                            nH = m1(2).values[0]
                            phoindex = m1(3).values[0]
                            norm = m1(5).values[0]

                            #Confidence intervals
                            nH_low = m1(2).error[0]
                            nH_up = m1(2).error[1]
                            phoindex_low = m1(3).error[0]
                            phoindex_up = m1(3).error[1]
                            norm_low = m1(5).error[0]
                            norm_up = m1(5).error[1]
                            beta, beta_up, beta_low = (np.nan, np.nan, np.nan)

                            #Confidence intervals 68%
                            xspec.Fit.error("stopat 1000,, maximum 1000.0 1.0 2,3,5") 
                            nH_low68 = m1(2).error[0]
                            nH_up68 = m1(2).error[1]
                            phoindex_low68 = m1(3).error[0]
                            phoindex_up68 = m1(3).error[1]
                            norm_low68 = m1(5).error[0]
                            norm_up68 = m1(5).error[1]
                            beta, beta_up68, beta_low68 = (np.nan, np.nan, np.nan)

                        if m1.expression=='constant*TBabs*zlogpar':
                            nH = m1(2).values[0]
                            phoindex = m1(3).values[0]
                            beta = m1(4).values[0]
                            norm = m1(7).values[0]

                            #Confidence intervals
                            nH_low = m1(2).error[0]
                            nH_up = m1(2).error[1]
                            phoindex_low = m1(3).error[0]
                            phoindex_up = m1(3).error[1]
                            beta_low = m1(4).error[0]
                            beta_up = m1(4).error[1]
                            norm_low = m1(7).error[0]
                            norm_up = m1(7).error[1]

                            #Confidence intervals 68%
                            xspec.Fit.error("stopat 1000,, maximum 1000.0 1.0 2,3,4,7") 
                            nH_low68 = m1(2).error[0]
                            nH_up68 = m1(2).error[1]
                            phoindex_low68 = m1(3).error[0]
                            phoindex_up68 = m1(3).error[1]
                            beta_low68 = m1(4).error[0]
                            beta_up68 = m1(4).error[1]
                            norm_low68 = m1(7).error[0]
                            norm_up68 = m1(7).error[1]

                        fit_statistic = xspec.Fit.statistic
                        test_statistic = xspec.Fit.testStatistic
                        dof = xspec.Fit.dof

                        # Retrieve rates and counts from xspec
                        src_rate = spectrum1.rate[0] + spectrum2.rate[0]  #net rate
                        src_rate_std = spectrum1.rate[1] + spectrum2.rate[1]
                        frac = spectrum1.rate[2] + spectrum2.rate[2] #total rate

                        src_cts = spectrum1.exposure*spectrum1.rate[0] + spectrum2.exposure*spectrum2.rate[0]
                        src_ects = spectrum1.exposure*spectrum1.rate[1] + spectrum2.exposure*spectrum2.rate[1] 
                        bkg_cts = (1. - frac/100) *src_cts
                        bkg_ects = (1. - frac/100) *src_ects

                        #Save output table
                        self.spectra_table.add_row((self.obsid,"rgs12", f"{expos0.expid}+{expos1.expid}", i, tstart, tstop,  exposure_time, m1.expression,
                                        nH, nH_low, nH_up, nH_low68, nH_up68,
                                        phoindex, phoindex_low, phoindex_up, phoindex_low68, phoindex_up68,
                                        beta, beta_low, beta_up, beta_low68, beta_up68, 
                                        norm, norm_low, norm_up, norm_low68, norm_up68,
                                        fit_statistic, test_statistic,
                                        dof, src_cts, src_ects, bkg_cts, bkg_ects, src_rate, src_rate_std,
                                        flux, flux_up, flux_low, lumin, lumin_up, lumin_low))

            # Close XSPEC's currently opened log file.
            xspec.Xset.closeLog()
            self.spectra_table.write(f'{self.target_dir}/Products/RGS_Spectra/{self.obsid}/{self.obsid}_table.fits', format='fits', overwrite=True)
            logging.info('Done spectral analysis. Please check your Products directory.')
        else:
            logging.info('Spectral analysis on pieces of spectrum already performed.')
    
    
    def vaughan_panel(self,  N, M, timescale=70, timebinsize=25):
        """
        Generates a variability plot, along the lines of those in Vaughan et al.

        :param N: number of bins to average on (from x to <x>)
        :type N: int
        :param M: number of bin to average on after having averaged on N bins
        :type N: int
        :param timescale: how long the duration of the lightcurve must be, in kiloseconds
        :type timescale: int
        :param timebinsize: bin size of lightcurve to insert as input parameter to rgslccorr, in seconds
        :type timebinsize: int
        """
        os.chdir(self.rgsdir)
        for i in range(self.npairs):
            if self.duration_lc_ks[i] >= timescale:
                logging.info('Observation long enough to make variability panel.')
                expos0 = Exposure(self.pairs_events[i][0], self.pairs_srcli[i][0])
                expos1 = Exposure(self.pairs_events[i][1], self.pairs_srcli[i][1])
                logging.info(f"Starting generation of Vaughan Panel for observation number {self.obsid}, exposures {expos0.expid}, {expos1.expid}.")

                #Run rgslccorr on timebin of 25s
                timebinsize = 25 #s
                if not glob.glob(os.path.join(self.rgsdir, f"{self.obsid}_{expos0.expid}+{expos1.expid}_bkg_rates_{timebinsize}bin.ds")):
                    logging.info(f"Running rgslccorr SAS command, timebinsize {timebinsize}s.")
                    rgslc_command = f"rgslccorr evlist='{expos0.evenli} {expos1.evenli}' srclist='{expos0.srcli} {expos1.srcli}' withbkgsubtraction=yes timebinsize={timebinsize} orders='1' sourceid=3 outputsrcfilename={self.obsid}_{expos0.expid}+{expos1.expid}_RGS_rates_{timebinsize}bin.ds outputbkgfilename={self.obsid}_{expos0.expid}+{expos1.expid}_bkg_rates_{timebinsize}bin.ds"
                    status_rgslc = run_command(rgslc_command)

                #Read LC data
                time, rate, erate, fracexp, backv, backe = mask_fracexp15(f"{self.rgsdir}/{self.obsid}_{expos0.expid}+{expos1.expid}_RGS_rates_{timebinsize}bin.ds")
                data = pd.DataFrame({'RATE': rate, "TIME": time, "ERROR": erate, 'BACKV': backv, "BACKE": backe})
                data = data[2:]
                if self.obsid == '0560980101':
                    data = data[1:]
                
                

                ###### --------------------------PANEL ----------------------------------#####
                fig, axs = plt.subplots(6, 1, figsize=(15,20), sharex=True, gridspec_kw={'hspace':0, 'wspace':0})
                fig.suptitle(f'RGS Lightcurve ObsId {self.obsid}, {expos0.expid}+{expos1.expid}, binsize {timebinsize}s \n N ={N}, M = {M}', fontsize=15, y=0.92)
                
                #---------Subplot x (lightcurve)---------#
                axs[0].errorbar(data['TIME'].values, data['RATE'].values, yerr=data['ERROR'].values, linestyle='', color='black', marker='.', ecolor='gray', 
                            label=f'RGS Lightcurve ObsId {self.obsid} binsize {timebinsize}s ')
                axs[0].grid(True)
                axs[0].set_ylabel('x', fontsize=10)


                #---------Subplot <x> (mean lightcurve)-----#
                mean_time, mean_data, mean_time_err, mean_error_data = binning(N, timebinsize, data, 'TIME', 'RATE' )

                axs[1].errorbar(mean_time, mean_data, yerr=mean_error_data, color='black', linestyle='', marker='.', ecolor='gray')
                axs[1].grid()
                axs[1].set_ylabel('<x>', fontsize=10)


                #----------Subplot excess variance---------#
                segment = N*timebinsize
                t_in = data['TIME'].values[0]
                t_fin = data['TIME'].values[-1]
                t = t_in
                xs_arr = []
                xs_err_arr = []
                
                while(t + segment < t_fin):

                    segment_df = data[(data['TIME']<t+segment) & (data['TIME']>t)]
                    n_in_segment = len(segment_df)
                    
                    if n_in_segment <=2:
                        t += segment
                        continue

                    else:
                        xs,err_xs = excess_variance(segment_df['RATE'].values, segment_df['ERROR'].values, normalized=False)
                        xs_arr.append(xs)
                        xs_err_arr.append(err_xs)
                        t+= segment

                
                mask_negative = []
                xs_uplim = []
        
                for i in range(len(xs_arr)):
                    if xs_arr[i]<0:
                        xs_arr[i] = xs_arr[i] + xs_err_arr[i]*1.64
                        xs_uplim.append(xs_arr[i] + xs_err_arr[i]*1.64)
                        mask_negative.append(False)

                    else:
                        mask_negative.append(True)

                xs_pos_arr = list(compress(xs_arr,mask_negative))
                xs_neg_arr = list(compress(xs_arr, np.invert(mask_negative)))
                xs_pos_err_arr = list(compress(xs_err_arr, mask_negative))
                mean_time_nonneg = list(compress(mean_time, mask_negative))
                mean_time_uplim = list(compress(mean_time, np.invert(mask_negative)))
                
                axs[2].errorbar(mean_time_nonneg, xs_pos_arr, xs_pos_err_arr, color='black', marker='.', linestyle='', ecolor='gray' )
                axs[2].errorbar(mean_time_uplim, xs_uplim, capthick=1, elinewidth=1, color='black', linestyle='', marker=r'$\downarrow$', uplims=True)
                axs[2].grid()
                axs[2].set_ylabel('$\sigma_{XS}^2$', fontsize=10)

                #----------Subplot mean excess variance----------#
                df_mean_xs = pd.DataFrame({'time': mean_time, 'xs': xs_arr})
                meanx2_times, mean_xs, meanx2_times_err, mean_xs_err = binning(M, N*timebinsize, df_mean_xs, 'time', 'xs')
                
                # Fit constant  
                xs_mean_arr = np.array(mean_xs)
                meanx2_times = np.array(meanx2_times)
                meanx2_times_err = np.array(meanx2_times_err)
                xs_err_mean_arr = np.array(mean_xs_err)
                avg_value_xs = np.mean(mean_xs)

                # Initial values
                initial_values =[avg_value_xs]
                pars, covm = curve_fit(constant, meanx2_times, xs_mean_arr, initial_values, xs_err_mean_arr) 

                q0 = pars    #parameter of fit
                dq = np.sqrt(covm.diagonal())   #and its error (from covariance matrix)

                # Print fit results
                print('Fit constant for excess variance:')
                print('q = %f +- %f' % (q0, dq))
                
                #chi2
                chisq =(((xs_mean_arr-constant(meanx2_times, q0) )/xs_err_mean_arr)**2).sum()
                ndof = len(meanx2_times) - 1
                print('Chisquare/ndof = %f/%d' % (chisq, ndof))

                conf90_positive = q0[0]*(10**(0.46)) # confidence intervals 90% given by Vaughan, 2003 (Table 1)
                conf90_negative = q0[0]*(10**(-0.78))
                conf99_positive = q0[0]*(10**(0.75)) # confidence intervals 99%
                conf99_negative = q0[0]*(10**(-1.16))

                axs[3].hlines(q0, t_in, t_fin, color='red', label=f"Constant fit: {q0[0]:.3f} +- {dq[0]:.3f} \n $\chi^2$/ndof = {chisq:.2f} / {ndof}")
                axs[3].errorbar(meanx2_times, mean_xs, mean_xs_err, xerr=meanx2_times_err,  linestyle='', color='black', marker='.', ecolor='gray')
                axs[3].hlines((conf90_positive, conf90_negative), t_in, t_fin, color='black', linestyle=':')
                axs[3].hlines((conf99_positive, conf99_negative), t_in, t_fin, color='black', linestyle='--')
                axs[3].grid()
                axs[3].legend(loc='lower right', fancybox=True)
                axs[3].set_yscale('log')
                axs[3].set_ylabel('$<\sigma_{XS}^2>$', fontsize=10)

                #-------------Subplot F_var-------------#
                segment = N*timebinsize
                t_in = data['TIME'].values[0]
                t_fin = data['TIME'].values[-1]
                t = t_in
                fvar_arr = []
                fvar_err_arr = []

                while(t + segment < t_fin):
                    
                    segment_df = data[(data['TIME']<t+segment) & (data['TIME']>t)]
                    n_in_segment = len(segment_df)
                    
                    if n_in_segment <=2:
                        t += segment
                        continue

                    else:
                        fvar, fvar_err = fractional_variability(segment_df['RATE'].values, segment_df['ERROR'].values, segment_df['BACKV'].values, segment_df['BACKE'].values, netlightcurve=True)
                        fvar_arr.append(fvar)
                        fvar_err_arr.append(fvar_err)
                        t += segment

                mask_fvar= []
                for el in fvar_arr:
                    if el==-1.:
                        mask_fvar.append(False)
                    else:
                        mask_fvar.append(True)
                fvar_arr = list(compress(fvar_arr, mask_fvar))
                fvar_err_arr = list(compress(fvar_err_arr, mask_fvar))
                
                df_fvar = pd.DataFrame({'time': mean_time, 'fvar':fvar_arr, 'fvar_err':fvar_err_arr})
                df_fvar = df_fvar.dropna()
                axs[4].errorbar(data=df_fvar, x='time', y='fvar', yerr='fvar_err', linestyle='', color='black', marker='.', ecolor='gray')
                axs[4].grid()
                axs[4].set_ylabel('$F_{var}$', fontsize=10)

                
                #---------------Subplot mean Fvar-----------#
                
                meanx2_times, fvar_mean_arr, meanx2_times_err, fvar_err_mean_arr = binning(M, N*timebinsize, df_fvar, 'time', 'fvar')

                axs[5].errorbar(meanx2_times, fvar_mean_arr, fvar_err_mean_arr, xerr=meanx2_times_err, linestyle='', color='black', marker='.', ecolor='gray')
                axs[5].grid()
                axs[5].set_xlabel('TIME [s]', fontsize=10)
                axs[5].set_ylabel('$<F_{var}>$', fontsize=10)

                # Fit constant  
                fvar_mean_arr = np.array(fvar_mean_arr)
                meanx2_times = np.array(meanx2_times)
                meanx2_times_err = np.array(meanx2_times_err)
                fvar_err_mean_arr = np.array(fvar_err_mean_arr)
                avg_value_fvar = np.mean(fvar_mean_arr)

                # Initial values
                initial_values =(avg_value_fvar)
                pars, covm = curve_fit(constant, meanx2_times, fvar_mean_arr, initial_values, fvar_err_mean_arr) 

                q0 = pars    #parameter of fit
                dq = np.sqrt(covm.diagonal())   #and its error (from covariance matrix)

                # Print fit results
                print('q = %f +- %f' % (q0, dq))
                
                #chi2
                chisq =(((fvar_mean_arr-constant(meanx2_times, q0) )/fvar_err_mean_arr)**2).sum()
                ndof = len(meanx2_times) - 1
                print('Chisquare/ndof = %f/%d' % (chisq, ndof))

                #90% confidence intervals 
                conf90_positive = q0[0]*10**(0.46/2.)
                conf90_negative = q0[0]*10**(-0.78/2.)
                conf99_positive = q0[0]*10**(0.75/2.)
                conf99_negative = q0[0]*10**(-1.16/2.)
                axs[5].hlines(q0, t_in, t_fin, color='red', label=f"Constant fit: {q0[0]:.3f} +- {dq[0]:.3f} \n $\chi^2$/ndof = {chisq:.2f} / {ndof}")
                axs[5].hlines((conf90_positive, conf90_negative), t_in, t_fin, color='black', linestyle=':')
                axs[5].hlines((conf99_positive, conf99_negative), t_in, t_fin, color='black', linestyle='--')
                axs[5].legend(loc='lower right', fancybox=True)
                axs[5].set_yscale('log')
                plt.savefig(f'{self.target_dir}/Products/Plots_timeseries/{self.obsid}_{expos0.expid}+{expos1.expid}_variability_panel.png')
                plt.close()
            
                ###------------------PLOT XS RATE CORRELATION--------------------###
                '''
                #Consider only positive values of xs
                i = 0
                segment = M*timebinsize
                t_in = data['TIME'].values[0]
                t_fin = data['TIME'].values[-1]
                t = t_in
                mean_rate = []
                mean_rate_err = []
                xs_arr_rate = []
                xs_arr_err_rate = []
                fvar_arr_rate = []
                fvar_arr_err_rate = []

                while(t + segment < t_fin):
                    
                    segment_df = data[(data['TIME']<t+segment) & (data['TIME']>t)]
                    n_in_segment = len(segment_df)
    
                    if n_in_segment <=2 :
                        t += segment
                        continue

                    else:
                        mean_rate.append(np.mean(segment_df['RATE'].values))
                        xs,err_xs = excess_variance(segment_df['RATE'].values, segment_df['ERROR'].values, normalized=False)
                        
                        fvar, fvar_err = fractional_variability(segment_df['RATE'].values, segment_df['ERROR'].values, segment_df['BACKV'].values, segment_df['BACKE'].values, netlightcurve=True)
                        xs_arr_rate.append(xs)
                        xs_arr_err_rate.append(err_xs)
                        fvar_arr_rate.append(fvar)
                        fvar_arr_err_rate.append(fvar_err)

                        t += segment

                xs_sorted = pd.DataFrame({'rate': mean_rate, 'xs': xs_arr_rate, 'xs_err': xs_arr_err_rate, 'fvar': fvar_arr_rate, 'fvar_err': fvar_arr_err_rate})
                xs_sorted = xs_sorted.dropna()
                xs_sorted = xs_sorted.sort_values(by=['rate'])
                xs_sorted = xs_sorted[xs_sorted['xs']>0.]   
                '''
                #Consider upper limits of xs in calculating fvar
                xs_sorted = pd.DataFrame({'rate': mean_data, 'xs': xs_arr, 'xs_err': xs_err_arr, 'fvar': fvar_arr, 'fvar_err': fvar_err_arr})
                xs_sorted = xs_sorted.dropna()
                xs_sorted = xs_sorted.sort_values(by=['rate'])
                

                #Binning
                meanx2_rate, meanx2_xs, meanx2_rate_err, meanx2_xs_err = binning(N, 1/N, xs_sorted, 'rate', 'xs')
                meanx2_rate, meanx2_fvar, meanx2_rate_err, meanx2_fvar_err = binning(N, 1/N, xs_sorted, 'rate', 'fvar')
                
                fig_rate, ax = plt.subplots(1,1, figsize=(10,10))
                ax.errorbar(x=meanx2_rate, y=meanx2_xs, yerr=meanx2_xs_err, xerr=meanx2_rate_err, linestyle='', marker='.')
                ax.set_xlabel('x')
                ax.set_ylabel('<$\sigma_{XS}^2$>')
                ax.set_xlim(0, max(meanx2_rate)+1)
                ax.grid(True)
                plt.savefig(f'{self.target_dir}/Products/Plots_timeseries/{self.obsid}_{expos0.expid}+{expos1.expid}_xs_rate.png')
                plt.close()

                #Save to csv file
                obs_array = np.ndarray(len(meanx2_xs))
                obs_array.fill(str(self.obsid))
                table_rate_xs = Table({'rate': meanx2_rate, 'erate': meanx2_rate_err, 'xs': meanx2_xs, 'xs_err': meanx2_xs_err, 'fvar': meanx2_fvar, 'fvar_err': meanx2_fvar_err, 'observation': obs_array}, dtype=('d', 'd', 'd', 'd', 'd', 'd', 'U9'))
                ascii.write(table=table_rate_xs, output=f'{self.target_dir}/Products/Plots_timeseries/{self.obsid}_{expos0.expid}+{expos1.expid}_rate_xs.csv', format='csv', overwrite=True)
